{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Topic extraction with Latent Dirichlet Allocation\n",
    "\n",
    "\n",
    "This is an example of applying :class:`sklearn.decomposition.LatentDirichletAllocation` on a corpus\n",
    "of documents and extract additive models of the topic structure of the\n",
    "corpus.  The output is a list of topics, each represented as a list of\n",
    "terms (weights are not shown).\n",
    "\n",
    "\n",
    "The default parameters (n_samples / n_features / n_components) should make\n",
    "the example runnable in a couple of tens of seconds. You can try to\n",
    "increase the dimensions of the problem, but be aware that the time\n",
    "complexity is proportional to (n_samples * iterations) in LDA.\n",
    "\n",
    "* http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/\n",
    "* https://stackoverflow.com/questions/20349958/understanding-lda-implementation-using-gensim\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: cute adopted broccoli munching look piece smoothie sister kittens like\n",
      "Topic #1: like broccoli adopted munching piece kittens sister look smoothie cute\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "# Exemplo 1 (scikit-learn lda)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pickle\n",
    "\n",
    "n_features = 10\n",
    "n_topics = 2\n",
    "n_top_words = 20\n",
    "\n",
    "# Print the n_top_words in order\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n",
    "# Training dataset\n",
    "data_samples = [\"I like to eat broccoli and bananas.\",\n",
    "                \"I ate a banana and spinach smoothie for breakfast.\",\n",
    "                \"Chinchillas and kittens are cute.\",\n",
    "                \"My sister adopted a kitten yesterday.\",\n",
    "                \"Look at this cute hamster munching on a piece of broccoli.\"\n",
    "               ]\n",
    "# extract fetures and vectorize dataset\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=1,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "\n",
    "#save features\n",
    "dic = tf_vectorizer.get_feature_names()\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "\n",
    "# train LDA\n",
    "p1 = lda.fit(tf)\n",
    "\n",
    "# Save all data necessary for later prediction\n",
    "# model = (dic,lda.components_,lda.exp_dirichlet_component_,lda.doc_topic_prior_)\n",
    "\n",
    "print_top_words(lda, dic, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Extracting tf features for LDA...\n",
      "\n",
      "Fitting LDA models with tf features, n_samples=2000 and n_features=1000...\n",
      "\n",
      "Topics in LDA model:\n",
      "Topic #0: edu com mail send graphics ftp pub available contact university list faq ca information cs 1993 program sun uk mit\n",
      "Topic #1: don like just know think ve way use right good going make sure ll point got need really time doesn\n",
      "Topic #2: christian think atheism faith pittsburgh new bible radio games alt lot just religion like book read play time subject believe\n",
      "Topic #3: drive disk windows thanks use card drives hard version pc software file using scsi help does new dos controller 16\n",
      "Topic #4: hiv health aids disease april medical care research 1993 light information study national service test led 10 page new drug\n",
      "Topic #5: god people does just good don jesus say israel way life know true fact time law want believe make think\n",
      "Topic #6: 55 10 11 18 15 team game 19 period play 23 12 13 flyers 20 25 22 17 24 16\n",
      "Topic #7: car year just cars new engine like bike good oil insurance better tires 000 thing speed model brake driving performance\n",
      "Topic #8: people said did just didn know time like went think children came come don took years say dead told started\n",
      "Topic #9: key space law government public use encryption earth section security moon probe enforcement keys states lunar military crime surface technology\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo 2 (scikit-learn lda)\n",
    "\n",
    "from __future__ import print_function\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 10\n",
    "n_top_words = 20\n",
    "\n",
    "\n",
    "# Print the n_top_words in order\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n",
    "\n",
    "# Load the 20 newsgroups dataset and vectorize it. We use a few heuristics\n",
    "# to filter out useless terms early on: the posts are stripped of headers,\n",
    "# footers and quoted replies, and common English words, words occurring in\n",
    "# only one document or in at least 95% of the documents are removed.\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "data_samples = dataset.data[:n_samples]\n",
    "\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "print()\n",
    "\n",
    "print(\"Fitting LDA models with tf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "\n",
    "lda.fit(tf)\n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizando ldamodel/gensim em um exemplo pequeno para entender o contexto\n",
    "\n",
    "\n",
    "Suponha que temos as seguintes sentenças: \n",
    "\n",
    "    I like to eat broccoli and bananas.\n",
    "    I ate a banana and spinach smoothie for breakfast.\n",
    "    Chinchillas and kittens are cute.\n",
    "    My sister adopted a kitten yesterday.\n",
    "    Look at this cute hamster munching on a piece of broccoli.\n",
    "\n",
    "Após passar elas por um pré-processamento no PreText2, teremos:\n",
    "\n",
    "    eat broccoli banana | \n",
    "    at banana spinach smoothi breakfast | \n",
    "    chinchilla kitten cute |\n",
    "    sister adopt kitten yesterdai | \n",
    "    cute hamster munch piec broccoli |\n",
    "    \n",
    "Desta forma, podemos rodar o LDA para descobrir a relação de 2 tópicos, um relacionado a alimentos e outro a animais fofos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tentativa mais próxima do satisfatório (Com um exemplo pequeno para entender o contexto)\n",
    "\n",
    "import numpy\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "QGS1 = open('/home/fuchs/Documentos/MESTRADO/Masters/Pre-Processing/Files/text-example_Maid/1.txt', 'r')\n",
    "QGS2 = open('/home/fuchs/Documentos/MESTRADO/Masters/Pre-Processing/Files/text-example_Maid/2.txt', 'r')\n",
    "QGS3 = open('/home/fuchs/Documentos/MESTRADO/Masters/Pre-Processing/Files/text-example_Maid/3.txt', 'r')\n",
    "QGS4 = open('/home/fuchs/Documentos/MESTRADO/Masters/Pre-Processing/Files/text-example_Maid/4.txt', 'r')\n",
    "QGS5 = open('/home/fuchs/Documentos/MESTRADO/Masters/Pre-Processing/Files/text-example_Maid/5.txt', 'r')\n",
    "\n",
    "\n",
    "# Criando variáveis com o conteudo dos arquivos\n",
    "textQGS1 = QGS1.read()\n",
    "textQGS2 = QGS2.read()\n",
    "textQGS3 = QGS3.read()\n",
    "textQGS4 = QGS4.read()\n",
    "textQGS5 = QGS5.read()\n",
    "\n",
    "\n",
    "# Juntando as variáveis em uma lista, onde cada componente é um documento\n",
    "docSet = [textQGS1, textQGS2, textQGS3, textQGS4, textQGS5]\n",
    "\n",
    "texts = []\n",
    "\n",
    "# Loop na lista de documentos\n",
    "for i in docSet:\n",
    "    \n",
    "    # Removendo as barras | do arquivo e separando por tokens\n",
    "    tokens = tokenizer.tokenize(i)    \n",
    "    \n",
    "    # Adiciona os tokens criados em uma lista\n",
    "    texts.append(tokens)\n",
    "\n",
    "# Transforma os documentos tokenizados em um id <-> termo do dicionário\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "    \n",
    "# Converte os documentos tokenizados em uma matriz documento/termo\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# Gerar o LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word = dictionary, passes=200)\n",
    "\n",
    "# Imprimir os tópicos em destaque\n",
    "print(ldamodel.print_topics(num_topics=2, num_words=3))\n",
    "\n",
    "# Fechando os arquivos\n",
    "QGS1.close()\n",
    "QGS2.close()\n",
    "QGS3.close()\n",
    "QGS4.close()\n",
    "QGS5.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizando ldamodel/gensim no problema real (Com o QGS pré-processado)\n",
    "\n",
    "\n",
    "Agora iremos generalizar o problema acima para o QGS da Revisão do Francisco.\n",
    "\n",
    "É notório dizer que a string de busca utilizada por ele foi:\n",
    "\n",
    "    ((\"software process improvement\") AND \n",
    "     (\"business  goal\" OR \"strategic\" OR \"goal oriented\" OR \"business oriented\" OR \"business strategy\")  AND\n",
    "     (\"alignment\" OR \"in line with\" OR \"geared to\" OR \"aligned with\" OR \"linking\")  AND\n",
    "     (\"method\" OR \"approach\" OR \"framework\" OR \"methodology\"))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imprimindo resultados: ['Porcentagem*'Termo']\n",
      "[(0, u'0.028*\"goal\" + 0.027*\"process\" + 0.022*\"improv\" + 0.017*\"softwar\" + 0.013*\"measur\" + 0.013*\"level\" + 0.013*\"busi\" + 0.010*\"organ\" + 0.009*\"defin\" + 0.009*\"requir\"')]\n",
      "\n",
      "\n",
      "Imprimindo resultados: ['Termo', Porcentagem]\n",
      "[(0, [(u'goal', 0.02824025), (u'process', 0.027116263), (u'improv', 0.022233931), (u'softwar', 0.01714085), (u'measur', 0.013382509), (u'level', 0.013101509), (u'busi', 0.013066383), (u'organ', 0.010326659), (u'defin', 0.009272919), (u'requir', 0.009272919)])]\n"
     ]
    }
   ],
   "source": [
    "# Tentativa mais próxima do satisfatório (Com o GQS pré-processado)\n",
    "\n",
    "import numpy\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# Abrindo os arquivos de texto (texts_Maids = Metadados e text_Maid = Texto Completo)\n",
    "QGS1 = open('/home/fuchs/Documentos/MESTRADO/Masters/Pre-Processing/Files/texts_Maid/QGS I.txt', 'r')\n",
    "QGS2 = open('/home/fuchs/Documentos/MESTRADO/Masters/Pre-Processing/Files/texts_Maid/QGS II.txt', 'r')\n",
    "QGS3 = open('/home/fuchs/Documentos/MESTRADO/Masters/Pre-Processing/Files/texts_Maid/QGS III.txt', 'r')\n",
    "QGS4 = open('/home/fuchs/Documentos/MESTRADO/Masters/Pre-Processing/Files/texts_Maid/QGS IV.txt', 'r')\n",
    "QGS5 = open('/home/fuchs/Documentos/MESTRADO/Masters/Pre-Processing/Files/texts_Maid/QGS V.txt', 'r')\n",
    "QGS6 = open('/home/fuchs/Documentos/MESTRADO/Masters/Pre-Processing/Files/texts_Maid/QGS VI.txt', 'r')\n",
    "QGS7 = open('/home/fuchs/Documentos/MESTRADO/Masters/Pre-Processing/Files/texts_Maid/QGS VII.txt', 'r')\n",
    "QGS8 = open('/home/fuchs/Documentos/MESTRADO/Masters/Pre-Processing/Files/texts_Maid/QGS VIII.txt', 'r')\n",
    "QGS9 = open('/home/fuchs/Documentos/MESTRADO/Masters/Pre-Processing/Files/texts_Maid/QGS IX.txt', 'r')\n",
    "QGS10 = open('/home/fuchs/Documentos/MESTRADO/Masters/Pre-Processing/Files/texts_Maid/QGS X.txt', 'r')\n",
    "\n",
    "\n",
    "# Criando variáveis com o conteudo dos arquivos\n",
    "textQGS1 = QGS1.read()\n",
    "textQGS2 = QGS2.read()\n",
    "textQGS3 = QGS3.read()\n",
    "textQGS4 = QGS4.read()\n",
    "textQGS5 = QGS5.read()\n",
    "textQGS6 = QGS6.read()\n",
    "textQGS7 = QGS7.read()\n",
    "textQGS8 = QGS8.read()\n",
    "textQGS9 = QGS9.read()\n",
    "textQGS10 = QGS10.read()\n",
    "\n",
    "# Juntando as variáveis em uma lista, onde cada componente é um documento\n",
    "docSet = [textQGS1, textQGS2, textQGS3, textQGS4, textQGS5, textQGS6, textQGS7, textQGS8, textQGS9, textQGS10]\n",
    "\n",
    "texts = []\n",
    "\n",
    "# Loop na lista de documentos\n",
    "for i in docSet:\n",
    "    \n",
    "    # Removendo as barras | do arquivo e separando por tokens\n",
    "    tokens = tokenizer.tokenize(i)    \n",
    "    \n",
    "    # Adiciona os tokens criados em uma lista\n",
    "    texts.append(tokens)\n",
    "\n",
    "# Transforma os documentos tokenizados em um id <-> termo do dicionário\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "    \n",
    "# Converte os documentos tokenizados em uma matriz documento/termo\n",
    "# A função doc2bow () simplesmente conta o número de ocorrências de cada palavra distinta, \n",
    "# converte a palavra em um id da palavra inteiro e retorna o resultado como um vetor esparso\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# Gerar o LDA model (https://radimrehurek.com/gensim/models/ldamodel.html)\n",
    "\n",
    "# gensim.models.ldamodel.LdaModel(corpus=None, num_topics=100, id2word=None, distributed=False, chunksize=2000, passes=1, \n",
    "#                                              update_every=1, alpha='symmetric', eta=None, decay=0.5, offset=1.0, \n",
    "#                                              eval_every=10, iterations=50, gamma_threshold=0.001, \n",
    "#                                              minimum_probability=0.01, random_state=None, ns_conf=None, \n",
    "#                                              minimum_phi_value=0.01, per_word_topics=False, callbacks=None, \n",
    "#                                              dtype=<type 'numpy.float32'>)\n",
    "\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=1, id2word = dictionary, passes=50)\n",
    "\n",
    "# Imprimir os tópicos em destaque\n",
    "print(\"Imprimindo resultados: ['Porcentagem*'Termo']\")\n",
    "print(ldamodel.print_topics(num_topics=1, num_words=10))\n",
    "print(\"\\n\")\n",
    "print(\"Imprimindo resultados: ['Termo', Porcentagem]\")\n",
    "print(ldamodel.show_topics(num_topics=1, num_words=10, log=False, formatted=False))\n",
    "\n",
    "# Fechando os arquivos\n",
    "QGS1.close()\n",
    "QGS2.close()\n",
    "QGS3.close()\n",
    "QGS4.close()\n",
    "QGS5.close()\n",
    "QGS6.close()\n",
    "QGS7.close()\n",
    "QGS8.close()\n",
    "QGS9.close()\n",
    "QGS10.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizando sklearn/LatentDirichletAllocation no problema real (Sem o QGS pré-processado)\n",
    "\n",
    "\n",
    "Agora iremos utilizar uma outra biblioteca para sanar o mesmo problema, porém sem utilizar os arquivos do QGS pré-processados.\n",
    "\n",
    "É notório dizer que a string de busca utilizada por ele foi:\n",
    "\n",
    "    ((\"software process improvement\") AND \n",
    "     (\"business  goal\" OR \"strategic\" OR \"goal oriented\" OR \"business oriented\" OR \"business strategy\")  AND\n",
    "     (\"alignment\" OR \"in line with\" OR \"geared to\" OR \"aligned with\" OR \"linking\")  AND\n",
    "     (\"method\" OR \"approach\" OR \"framework\" OR \"methodology\"))\n",
    "\n",
    "Funções utilizadas:\n",
    "\n",
    "**CountVectorizer** (input = ’content’, encoding = ’utf-8’, decode_error = ’strict’, strip_accents = None, lowercase = True, preprocessor = None, tokenizer = None, stop_words = None, token_pattern = ’(?u)\\b\\w\\w+\\b’, ngram_range = (1, 1), analyzer = ’word’, max_df = 1.0, min_df = 1, max_features = None, vocabulary = None, binary = False, dtype = <class ‘numpy.int64’>)\n",
    "\n",
    "Convert a collection of text documents to a matrix of token counts.\n",
    "\n",
    "*input : string {‘filename’, ‘file’, ‘content’}*\n",
    "\n",
    "If ‘filename’, the sequence passed as an argument to fit is expected to be a list of filenames that need reading to fetch the raw content to analyze. If ‘file’, the sequence items must have a ‘read’ method (file-like object) that is called to fetch the bytes in memory. Otherwise the input is expected to be the sequence strings or bytes items are expected to be analyzed directly.\n",
    "\n",
    "*encoding : string, ‘utf-8’ by default.*\n",
    "\n",
    "If bytes or files are given to analyze, this encoding is used to decode.\n",
    "\n",
    "*decode_error : {‘strict’, ‘ignore’, ‘replace’}*\n",
    "\n",
    "Instruction on what to do if a byte sequence is given to analyze that contains characters not of the given encoding. By default, it is ‘strict’, meaning that a UnicodeDecodeError will be raised. Other values are ‘ignore’ and ‘replace’.\n",
    "\n",
    "*strip_accents : {‘ascii’, ‘unicode’, None}*\n",
    "\n",
    "Remove accents and perform other character normalization during the preprocessing step. ‘ascii’ is a fast method that only works on characters that have an direct ASCII mapping. ‘unicode’ is a slightly slower method that works on any characters. None (default) does nothing. Both ‘ascii’ and ‘unicode’ use NFKD normalization from unicodedata.normalize.\n",
    "\n",
    "*lowercase : boolean, True by default*\n",
    "\n",
    "Convert all characters to lowercase before tokenizing.\n",
    "\n",
    "*preprocessor : callable or None (default)*\n",
    "\n",
    "Override the preprocessing (string transformation) stage while preserving the tokenizing and n-grams generation steps.\n",
    "\n",
    "*tokenizer : callable or None (default)*\n",
    "\n",
    "Override the string tokenization step while preserving the preprocessing and n-grams generation steps. Only applies if analyzer == 'word'.\n",
    "\n",
    "*stop_words : string {‘english’}, list, or None (default)*\n",
    "\n",
    "If ‘english’, a built-in stop word list for English is used. There are several known issues with ‘english’ and you should consider an alternative (see Using stop words). If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens. Only applies if analyzer == 'word'. If None, no stop words will be used. max_df can be set to a value in the range [0.7, 1.0) to automatically detect and filter stop words based on intra corpus document frequency of terms.\n",
    "\n",
    "*token_pattern : string*\n",
    "\n",
    "Regular expression denoting what constitutes a “token”, only used if analyzer == 'word'. The default regexp select tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator).\n",
    "\n",
    "*ngram_range : tuple (min_n, max_n)*\n",
    "\n",
    "The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used.\n",
    "\n",
    "*analyzer : string, {‘word’, ‘char’, ‘char_wb’} or callable* \n",
    "\n",
    "Whether the feature should be made of word or character n-grams. Option ‘char_wb’ creates character n-grams only from text inside word boundaries; n-grams at the edges of words are padded with space. If a callable is passed it is used to extract the sequence of features out of the raw, unprocessed input.\n",
    "\n",
    "*max_df : float in range [0.0, 1.0] or int, default=1.0*\n",
    "\n",
    "When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "*min_df : float in range [0.0, 1.0] or int, default=1*\n",
    "\n",
    "When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "*max_features : int or None, default=None* \n",
    "\n",
    "If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus. This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "*vocabulary : Mapping or iterable, optional*\n",
    "\n",
    "Either a Mapping (e.g., a dict) where keys are terms and values are indices in the feature matrix, or an iterable over terms. If not given, a vocabulary is determined from the input documents. Indices in the mapping should not be repeated and should not have any gap between 0 and the largest index.\n",
    "\n",
    "*binary : boolean, default=False*\n",
    "\n",
    "If True, all non zero counts are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts.\n",
    "\n",
    "*dtype : type, optional*\n",
    "\n",
    "Type of the matrix returned by fit_transform() or transform().\n",
    "\n",
    "<br>\n",
    "\n",
    "**.fit_transform(raw_documents, y=None)**\n",
    "\n",
    "Learn the vocabulary dictionary and return term-document matrix. This is equivalent to fit followed by transform, but more efficiently implemented.\n",
    "\n",
    "Parameters:\t\n",
    "*raw_documents : iterable*\n",
    "\n",
    "An iterable which yields either str, unicode or file objects.\n",
    "\n",
    "Returns:\t\n",
    "*X : array, [n_samples, n_features]*\n",
    "\n",
    "Document-term matrix.\n",
    "\n",
    "<br>\n",
    "\n",
    "**.get_feature_names()**\n",
    "\n",
    "Array mapping from feature integer indices to feature name\n",
    "\n",
    "<br>\n",
    "\n",
    "**LatentDirichletAllocation** (n_components = 10, doc_topic_prior = None, topic_word_prior = None, learning_method = ’batch’, learning_decay = 0.7, learning_offset = 10.0, max_iter = 10, batch_size = 128, evaluate_every = -1, total_samples = 1000000.0, perp_tol = 0.1, mean_change_tol = 0.001, max_doc_update_iter = 100, n_jobs = None, verbose = 0, random_state = None, n_topics = None)\n",
    "\n",
    "*n_components : int, optional (default = 10)*\n",
    "\n",
    "Number of topics.\n",
    "\n",
    "*doc_topic_prior : float, optional (default=None)*\n",
    "\n",
    "Prior of document topic distribution theta. If the value is None, defaults to 1 / n_components. In the literature, this is called alpha.\n",
    "\n",
    "*topic_word_prior : float, optional (default=None)*\n",
    "\n",
    "Prior of topic word distribution beta. If the value is None, defaults to 1 / n_components. In the literature, this is called beta.\n",
    "\n",
    "*learning_method : ‘batch’ | ‘online’, default=’batch’*\n",
    "\n",
    "Method used to update _component. Only used in fit method. In general, if the data size is large, the online update will be much faster than the batch update.\n",
    "\n",
    "Valid options:\n",
    "\n",
    "'batch': Batch variational Bayes method. Use all training data in each EM update. Old components_ will be overwritten in each iteration.\n",
    "'online': Online variational Bayes method. In each EM update, use mini-batch of training data to update the components_ variable incrementally. The learning rate is controlled by the learning_decay and the learning_offset parameters.\n",
    "\n",
    "*learning_decay : float, optional (default=0.7)*\n",
    "\n",
    "It is a parameter that control learning rate in the online learning method. The value should be set between (0.5, 1.0] to guarantee asymptotic convergence. When the value is 0.0 and batch_size is n_samples, the update method is same as batch learning. In the literature, this is called kappa.\n",
    "\n",
    "*learning_offset : float, optional (default=10.)*\n",
    "\n",
    "A (positive) parameter that downweights early iterations in online learning. It should be greater than 1.0. In the literature, this is called tau_0.\n",
    "\n",
    "*max_iter : integer, optional (default=10)*\n",
    "\n",
    "The maximum number of iterations.\n",
    "\n",
    "*batch_size : int, optional (default=128)*\n",
    "\n",
    "Number of documents to use in each EM iteration. Only used in online learning.\n",
    "\n",
    "*evaluate_every : int, optional (default=0)*\n",
    "\n",
    "How often to evaluate perplexity. Only used in fit method. set it to 0 or negative number to not evalute perplexity in training at all. Evaluating perplexity can help you check convergence in training process, but it will also increase total training time. Evaluating perplexity in every iteration might increase training time up to two-fold.\n",
    "\n",
    "*total_samples : int, optional (default=1e6)*\n",
    "\n",
    "Total number of documents. Only used in the partial_fit method.\n",
    "\n",
    "*perp_tol : float, optional (default=1e-1)*\n",
    "\n",
    "Perplexity tolerance in batch learning. Only used when evaluate_every is greater than 0.\n",
    "\n",
    "*mean_change_tol : float, optional (default=1e-3)*\n",
    "\n",
    "Stopping tolerance for updating document topic distribution in E-step.\n",
    "\n",
    "*max_doc_update_iter : int (default=100)*\n",
    "\n",
    "Max number of iterations for updating document topic distribution in the E-step.\n",
    "\n",
    "*n_jobs : int or None, optional (default=None)*\n",
    "\n",
    "The number of jobs to use in the E-step. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. See Glossary for more details.\n",
    "\n",
    "*verbose : int, optional (default=0)*\n",
    "\n",
    "Verbosity level.\n",
    "\n",
    "*random_state : int, RandomState instance or None, optional (default=None)*\n",
    "\n",
    "If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.\n",
    "\n",
    "*n_topics : int, optional (default=None)*\n",
    "\n",
    "This parameter has been renamed to n_components and will be removed in version 0.21. .. deprecated:: 0.19\n",
    "\n",
    "\n",
    "**.fit(X, y = None)**\n",
    "\n",
    "Learn model for the data X with variational Bayes method. When learning_method is ‘online’, use mini-batch update. Otherwise, use batch update.\n",
    "Parameters:\t\n",
    "\n",
    "*X : array-like or sparse matrix, shape=(n_samples, n_features)*\n",
    "\n",
    "Document word matrix.\n",
    "\n",
    "*y : Ignored*\n",
    "\n",
    "Returns: None\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.51998236  0.51659844  0.51756236  0.51722316  0.51940122  0.51637261\n",
      "   0.51724284  0.52097552  0.51846673  0.51766711  0.51998236  0.51621688\n",
      "   0.51465192  0.52076596  0.52276839  0.52276839  0.52105388  0.52037802\n",
      "   0.51702257  0.51624633  0.51615773  0.51653754  0.51676433  0.51571712\n",
      "   0.52007954  0.51691658  0.51504855  0.51683174  0.51653211  0.52356185\n",
      "   0.51940413  0.52215982]\n",
      " [10.48001764  4.48340156 15.48243764 11.48277684 23.48059878  5.48362739\n",
      "  11.48275716 10.47902448 17.48153327 35.48233289 10.48001764  5.48378312\n",
      "   8.48534808  4.47923404  5.47723161  5.47723161 21.47894612 12.47962198\n",
      "  10.48297743  9.48375367  5.48384227 37.48346246 22.48323567  7.48428288\n",
      "   8.47992046 58.48308342 10.48495145 17.48316826 15.48346789  9.47643815\n",
      "  10.48059587  6.47784018]]\n",
      "Topic 1: {strategies - linking software - linking - success - measurement - goal - levels - organization - results - keywords}\n",
      "\n",
      "Topic 2: {software - process - improvement - business - process improvement - measurement - software process - goals - software process improvement - approach}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.datasets import load_files\n",
    "\n",
    "number_topics = 2\n",
    "number_words = 10\n",
    "max_document_frequency = 1.0\n",
    "min_document_frequency = 0.4\n",
    "ngram = (1,3)\n",
    "max_features = None\n",
    "\n",
    "alpha = None\n",
    "beta = None\n",
    "learning = 'batch' # Bacth or Online\n",
    "iterations = 500\n",
    "\n",
    "# Print the topics with the words in order\n",
    "def print_top_words(model, feature_names, number_words):\n",
    "    for topic_index, topic in enumerate(model.components_):\n",
    "        message = \"Topic %d: \" % (topic_index + 1)\n",
    "        message += \"{\"\n",
    "        message += \" - \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-number_words - 1:-1]])\n",
    "        message += \"}\\n\"\n",
    "        print(message)\n",
    "\n",
    "# Load training dataset\n",
    "files = load_files(container_path = '/home/fuchs/Documentos/MESTRADO/Masters/Files-QGS/QGS-txt', encoding = \"iso-8859-1\")\n",
    "\n",
    "\n",
    "# Extract words and vectorize dataset\n",
    "tf_vectorizer = CountVectorizer(max_df = max_document_frequency,\n",
    "                                min_df = min_document_frequency,\n",
    "                                ngram_range = ngram,\n",
    "                                max_features = max_features,\n",
    "                                stop_words = 'english')\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(files.data)\n",
    "\n",
    "# Save word names in a dicionary\n",
    "dic = tf_vectorizer.get_feature_names()\n",
    "\n",
    "# Execute lda and training\n",
    "lda = LatentDirichletAllocation(n_components = number_topics,\n",
    "                                doc_topic_prior = alpha,\n",
    "                                topic_word_prior = beta,\n",
    "                                learning_method = learning, \n",
    "                                learning_decay = 0.7,\n",
    "                                learning_offset = 10.0,\n",
    "                                max_iter = iterations,\n",
    "                                batch_size = 128,\n",
    "                                evaluate_every = -1,\n",
    "                                total_samples = 1000000.0,\n",
    "                                perp_tol = 0.1,\n",
    "                                mean_change_tol = 0.001,\n",
    "                                max_doc_update_iter = 100,\n",
    "                                random_state = None)\n",
    "lda.fit(tf)\n",
    "\n",
    "print(lda.components_)\n",
    "# Print the topics (number_topics) with the words (number_words)\n",
    "\n",
    "print_top_words(lda, dic, number_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
