2015 13th International 

Conference on Document Analysis and Recognition 

(ICDAR) 

DASyR(IR) 
Systematic 

Analysis System for 
Reviews (in Information 

- Document 

Retrieval) 

Flarina Piroi, Aida Lipani, Mihai Lupu, and Allan Hanbury 

Institute 

of Software 

Engineering 

and Information 

Systems 

Vienna University 
9-111188, 

Strasse 

Favoriten 

of Technology 

Vienna 1040, Austria 

Email: (lastname)@ifs.tuwien.ac.at 

For the  review  au­

document analysis system. 

systematic reviews  is  a 

in domains where experimental 

To support the creation of systematic 

painstaking 
task 
results are 

Abstract-Creating 
undertaken especially 
the primary method to knowledge  creation. 
thors, analysing documents to extract relevant data is  a demand­
ing activity. 
reviews, we have 
created  DASyR-a semi-automatic 
DASyR is our solution to annotating published papers for the 
purpose of ontology population. 
are  not existing or inadequate, 
annotation bootstrapping 
Indexing, followed by traditional 
extend  the  annotation 
application 
to  a  subdomain 
Retrieval evaluation 
perfect domain to  test on. 
scale  experimental 
results for 
We show the utility of DASyR through experimental 
evaluated 
different 
parameter values for the bootstrap procedure, 
in  terms of annotator agreement, error rate, precision and recall. 

Machine Learning algorithms to 
set. We provide an example of the method 

domain. T he  reliance 
studies makes  it  a 

For domains where dictionaries 
semi-automatic 

method based on positional 

of Computer Science, the  Information 

of this domain on  large 

DASyR relies  on  a 

Random 

I. INTRODUCTION 

have to be 

systematic 

In knowledge communication, 
most prominent 

instrument, 

reviews are an 
in the medical do­

view about the 

to assisting 

of thousands 

review thousands 

on a specific topic of interest. 

gain [4] and are mostly limited 

is increasing
so the practice 

essential 
main [1]. Here, for realising a systematic 
[2] of documents 
and even hundreds 
analysed in order to create a comprehensive 
state of the evidence 
Software 
tools to assist with this process have been developed already 
since the mid 1990s [3]. Such tools have, however, slow 
acceptance 
with the 
logistics 
of the work (like note taking and document manage­
ment) [5]. Empirical evidence 
other than medical sciences, 
views becomes attractive 
or computer 
to support a wider range of users and domains, 
the vast majority 
vocabulary, even less so dictionaries 
we have developed DASyR-a Document Analysis System for 
Systematic 
retrieva
essentially 
benchmarking 
retrieva
that allows the user to dynamically create a domain dictionary 
(or populate 
analysis of the corpus at hand, and DASyR(IR) is the first 
application 

with the need 
for which, in 
of cases, there is no commonly agreed upon 

Retrieval (IR) is 
and 
l adoption 
of a 

ts. Information 
science,  where 

l model or system. DASyR is the first such system 

Reviews-and 
l  resul

a domain ontology) based on a  latent 

science [7]. We are then confronted 

in areas like software engineering 

applied it to the study of information 

experimentation 

on information 

to the successfu

l experimenta

l documents. 

an empirical 

In this context 

is paramount 

or thesauri. 

semantic 

retrieva

[6] 

ly  larger in fields 
of systematic  re­

A. Motivation 

A number of campaigns 

for the evaluation of IR methods 

the 

testing 

provide 

reports. 

judgements) 

and relevance 

document collections 

since 20001. TREC2 

across numerous scientific 

from all CLEF conferences 

archive of evaluation tracks and 
currently 
since its beginnings, 
NTCIR3, too, has about 2000 reports 

The result is a rich body of knowledge, 
For instance, 
has published over 1,500 pages of evalua­

are running on a yearly cycle. These campaigns 
tools (queries, 
to all researchers. 
distributed 
CLEF Association 
tion reports 
keeps a well maintained 
reports 
participants' 
of over 1,420 papers. 
its website. 
such as FIRE4 or MediaEval5, we can say that 1) there is 
considerab
of IR systems in 
and 2) there are very few, if any, 
a wide variety 
comprehensive 
[8], [9]. The reason for such a  limited number of horizonta
studies is that putting this existing 
body of experience 
work-in academic research 
significant 
of various participants. 
to date with a  large amount of literature. 

of domains; 
horizonta

Adding to these the new evaluation 

A novice will need to quickly get up 

l practice-takes 
effort and is generally done through the experience 

about the performance 

across the existing 

le literature 

consisting 

campaigns, 

or industria

l 
to 
a 

resources 

l studies 

on 

In general, it has been observed 

that when the information 

need is to be satisfied 
documents, 
information 

as is the case for systematic 
access systems fail  [10]. 

by pieces of information 
reviews, 

in various 
current 

DASyR addresses 
its instantiation 

for the IR domain. 

this issue and, in this report, 

we show 

B. Related Work 

To automate document analysis, we must understand 

data we have and annotate 
the following questions 

ly. Setting 
are the most immediate 

it according

the 
on this path 
to answer: 

1) what kind of and which entities 
2) how can this be done efficiently and effective

should be annotated? 

ly? 

To the best of our knowledge, the  system  most 

simi­

lar to DASyR was recently presented 
and col­
leagues [11]. Like other document analysis systems [12], [13] 

by Schuster 

gov/ 

1 http://www.clef-initiative.eu 
2 http://trec.nist.
3 http://research.n 
I 
4http://www.isical.ac.
5 http://www.multimediaeval.or

i i .ac.jp/ntcir 

in/�clial 

g/ 

978-1-4799-1805-8/15/$31.00 
©2015 IEEE 

591 

2015 13th International 

Conference on Document Analysis and Recognition 

(ICDAR) 

Information 

and 

exploration 
extraction 
(Figure 

2) 

document 
collection 

OCR 
(i� 
'-' 

Lucene 

XML Text 
Inverted 
repository (2� file index 

'-' 

PDF collection 

(i� Random 
'-' 

Indexing 

0 

® Visualisation 

Fig. 1. DASyR  system outline 

GUI 

(Figure 3) 

receipts) 

received, 

a predefined 

and extracts 

reviews problem does not benefit from a 

(i.e. 
set of metadata 
date, amount, customer 

however, it focuses on a specific kind of documents 
invoices, 
(e.g. document type, sender, 
ID). The systematic 
predefined 
there is no dictionary 
We can make use of an ontology 
domain, but using such an ontology 
problem. 
and we do so here, is which entities 

to a 
is only partly solving 
the 

The more challenging part, which is not yet addressed 

agreed upon by the research 

set of fields to extract, 

should be annotated. 

cOlmn unity. 

and, even more problematic, 

of concepts  particular 

To scale up the annotation 

there is a substantial 
amount 
Extraction 
(IE) 

techniques 

are still efficient 

in the Information 

prop­
tf-idf). 
Here 

Named Entity Recognition 

Lee et al. [15] compare two NER algorithm 

work described 
In particular, 

(NER) is 
[14], both making use 

of previous 
literature. 
done following two main paradigms 
to extend the set of extracted 
of machine learning 
terms. The first paradigm is based on the distributional 
erties of the concept terms (e.g. term frequency, 
Support Vector Machines (SVM) approaches 
and effective: 
families, modified structural 
SVMs and Conditiona
Fields (CRF), finding that the former outperforms 
reduced training 
textual information 
by Gildea and Jurafsky 
more recently, 
paradigms 
The first paradigm is exemplified by an SVM classification 
where the manual annotations 
II-B), while the second is concretised 
vector space and decision 

l Random 
CRF and has 
is based on con­

set (Section 
by the use of a semantic 

by Yimam et al  [17]. DASyR uses both of these 

terms, as used 
[16], or, 

times. The second paradigm 

document analysis 

into one coherent 

roles annotation 

with the selected 

are the training 

trees (Sections 

associated 

for semantic 

framework. 

II-A, II-B). 

In what follows, Section II describes 

our system architec­

ture, Section III shows the results 
the IR data collection, 
an outlook for the system. 

of applying the system on 

and Section IV summarises, 

providing 

II. SYSTEM DESCRIPTION 

DASyR has two parts  (Figure 

1): an information 
explo­
component with which the user inter­

component 

II-A, Figure 2), and an automated 

ration and extraction 
acts (Section 
extraction 
system mentioned 
to perform OCR (step 1 in Figure 2). The output of this step 
is stored as X ML data and in what follows we shall refer to 
this X ML set as the "data collection" 

II-B). Similarly to the Intellix 

or the "data set". 

(Section 

information 

above [II], DASyR relies on existing tools 

A. Bootstrapped manual annotations 

In any information 

system the 
main design challenge is giving users the possibility to anno­
tate as many term instances 

and extraction 

exploration 

in a short time. 

as possible 

Fig. 2. Annotation 

system workflow 

Therefore, 

before a user can interact 
is used in step 2 of Figure 2 to create: 

with the system, the 

data collection 

1) a semantic 

vector space, based on positiona

l Random 
(pRI) [18]. The pRI engine allows to iden­

Indexing 
tify terms related 
semantic 
given 'flu' identify 

to a given term according 

to their 

similarity (e.g. different 

types of diseases: 

2) an inverted 

based on Lucene [19], 

'cold'). 
file search engine, 

to provide the user with a surrounding 
for the automatica

textual 
terms. 

lly identified 

related 

context 

documents is problematic 

procedure 

Asking users to freely annotate 
because of the huge time requirements 
end annotation 
a closed-set 
text annotated 
significantly 
an existing 

and only approves or rejects 
However, the closed-set 

faster. 
dictionary 

on unstructured 

approach (one where a user already sees the 
is 

and is limited to that dictionary. 

imposed by an open­

documents. 

annotations) 
requires 

Instead, 

approach 

To overcome the lack of domain dictionaries, 

DASyR 
some minimal input from the user. This is a small 

requires 
set of examples of the concepts 
collection. 
semantic 
to create a dictionary 
the ongoing use of the system. 

In DASyR we refer to these examples 

to be extracted 

from the data 
as seeds. The 

vector space created by the pRI component 

is used 

that is continuously 

updated based on 

in addition 

by the user in a class, 

in the indexed dataset 

For the terms annotated 

them in a domain specific dictionary 

to 
(step 6 in Figure 2, 
a number, N, of new 
of the 
representatives 
of these terms are then 

storing 
vector space engine returns 
the semantic 
terms. These are candidates 
to be further 
class (step 3 in Figure 2). All instances 
identified 
in turn, sends its output to a pool of terms and their contexts 
(step 4 in Figure 2). The GUI will present to the user terms 
out of this pool, one at a time, together 
(step 5), for the user to annotate 
the term context 
phrases 
preprocessing 
in order to maintain 

to the user is at phrase level, where 
The term 
using only the S-Stemmer 
[20], 

the flexibility of the system as  a whole. 

(step 6). The granularity 

by the search engine which, 

using a simple sentence 

is conservative, 

with their context 

are identified 

presented 

splitter. 

of 

In addition 

to the classes 

of interest 

the user the system offers the annotators 

initially selected 
two further 

special 

by 

592 

2015 13th International 

Conference on Document Analysis and Recognition 

(ICDAR) 

TABLE l. CLASSES IN THE CURRENT INSTANTIATION 

OF DASyR 

A more significant 

decline in performance is detected between runs 

706 and 707 (switching 
field to a manual  edit), 

from automatic filtering 
at the  .025  level.  In  our 
training, 

_ the best results for R-Prec and� , measures which were 
degrees for TREC-2006.� esult was duplicated 

of the RequestText 

run  605 

in 

used  to differing 
the runs for TREe-Z007. 

Last 1 00 garbaged 

words 

documents 

Collection a set of 
a Collection enriched 
Test Collection 
Task 
a sel of 
Challenge a sel of 
Run 
Measure a function 

the answer of a system to a Task 

modelling 
tasks 

queries 
relaled 

with queries 
a specific 

and relevance 
seeking 
information 
task 

judgements 

to give a run a numeric 

value indicative 

of quality 

o NTCIR-EVIA 
• CLEF 
• Research 

Articles 

Fig. 3. Annotation 

system graphical 

user interface 

400 
350 !i\lITREC 
'" � 300 
E 
a 250 
o 
::: 200 
o � 150 
� 100 
z 
50 
That is, when the annotator 
o 

process by ex­

instance. 

can be chosen 

expertise. 

a presented 

that the presented 

with the 
pipeline. 

lead to reconsidering 

None and Garbage. The first one is used 

term can never be an instance 
the term. 

concept classes: 
when none of the user given concept classes 
to annotate 
A term annotated 
Garbage class is removed from the annotation 
Garbage is used to speedup the annotation 
ploiting the annotator's 
is confident 
any of the initial concept classes, he or she 'garbages' 
If new term instances 
annotators 
to another concept class. This can be done in two ways: 
extemporaneous
presented 
via annotation 
shown context 
helps the user realise whether a garbaged term is actually 
not Garbage in that particular context. 
the annotation 
Garbage by at least two users are not shown to the other 
users anymore, thus improving 

ly, 
highlights of all the garbaged words in the 
of a target term. This latter 

speed up 
all words that have been marked as 

ly, via the list of the last 100 garbaged 

the user experience. 

in the interface 

visualisation 

To further 

process, 

of 

tokens 

(Figure 3), or contemporaneous

do have the possibility to change the annotation 

feature 

a garbaged  term, 

B. Automatic annotation 

Starting 

from the manually created annotations, 
DASyR 

using Machine 

in Section I-B, the ML task 

from two angles: 1) classifying 

expands to the entire set of possible entities 
Learning (ML). As discussed 
is approached 
a specific class  fo
llowing the first paradigm mentioned 
Maynard [14], using SVM; and 2) for each term previously 
annotated 
of the annotators, 
None or as an ontology class, using the contextua
implemented by decision 

other than None or Garbage by one 

each token in 
by 

all of its occurrences 

classifying 

as something 

trees. 

as either 

l paradigm, 

To generate 

training 

features, 

DASyR relies on a popular 

text engineering 
form (i.e. the term itself), 
initiaIUpperCased), 
10 tokens  around 

each token. 

toolkit, GATE [21], to extract the token 

type (e.g. number, lowercased, 

and part-of-speech 

tag, for a window of 

For the first approach 

(classification 

of all tokens)  DASyR 

a parameter 

kernel SVM (RBF-SVM) [22] 
scan to identify 
the I 

of the RBF, as well as the tradeoff between training 
j, by which training 
scan, it uses the �alpha 

uses a radial basis  function 
for which it first performs 
parameter 
error and margin, 
errors on positive 
amples. To speed up the parameter 
estimates 
and polynomial kernels were also explored, as well  as the 
with Uneven Margins method, with worse results. 
Perceptron 
When multiple classes are to be detected, 
as it would generally 

c, and the cost factor, 
examples outweigh errors on negative 

by the SVMLight6 implementation. 
Linear 

generated 

ex­

6http://svmli

ght.joachims.or

g! 

Fig. 4. Dataset size and publication 
collection 

venues of the Research 

Articles 

sub 

be the case, the multi-classification 
classification 

tasks using a one-vs-aU 

approach. 

problem is split into binary 

For the second approach 

(classification 

of specific terms) 

of the decision 
tree 
DASyR uses the J48 [23] implementation 
classifier 
on the 
C4.5, which has the benefit of giving insights 
data by legible trees, whence it is possible to produce rules. 

III. EVALUATION AND USE OF ANNOTATIONS 

To demonstrate 

DASyR, we instantiated 

We refer to it as DASyR(IR). 

it for a horizonta
retrieva
We created 
reports 

reports. 
based on the TREC, CLEF and NTCIR/EvIA 

l 
l 
survey of evaluation metrics used in information 
research 
a dataset 
available on their respective 
of 687 research 
(i.e. without crawling publishers' 
the document distribution 
terms in the publication 
dataset 
uses approximate

We also added a set 
articles in the IR field from a private 
set 
Figure 4 shows 
across years and the most common 
venues of the research 
ly 17GBs hard disk space. 

websites). 

websites. 

articles. The 

The set of concept classes 

in the annotation 

process were 
by Lipani et al.  [24]. 
the set to six classes 

we restricted 

next, the performance 

chosen from the IR ontology proposed 
For this pilot evaluation 
(Table I), all part of the Evaluation category 
ogy. We observe, 
exploration and extraction 
of the individua
bootstrapping 
N, the number of terms generated 
classified 
annotation 
of DASyR(IR). 

term. We then look at annotation 
results, and show some statistics 

method behaves depending 

l components. 

task as a whole  as well  as that 

We analyse first how the pRI 
on the choice of 

based on each newly 
agreements, 

final 

enabled by the use 

in the ontol­

of the information 

The annotation 

29585 annotations. 

was performed 
The average annotation 

by five users, who generated 

time was 2.71s per 

593 

2015 13th International 

Conference on Document Analysis and Recognition 

(ICDAR) 

the computer latency, 

term. Considering 
this resulted in around 35 person-hours 
annotations 
GATE component 
the automated 

generated 
information 

and based on the existing 

extraction 

engine. 

which was of 1.3s, 
of effort. Using these 
pool of documents, 

the 

9008455 features 

to be used by 

A. Random Indexing bootstraping 

as instances 

An a posteriori 

a restart 

The question 

(many terms) search. 

We can think of this as a depth-first 

we need to answer here is whether pRI should 
fewer or more similar terms every time a new class 
is annotated. 
(few 

generate 
instance 
terms) vs. breadth-first 
analysis, where the valid terms are those annotated 
of a class by a user, shows that for small candidates 
(2 :s; N :s; 5), pRI stops generating 
new terms very soon, and 
would require 
with new, externally introduced 
At the same time, the more candidates 
at each 
we introduce 
step (larger N), the lower the proportion 
of valid terms is. This 
behaviour 
in figures 5 and 6. Figure 5 shows the 
for 1 :s; N :s;  20, with 
cumulative number of valid suggestions 
highlighted lines for N= 2, 5, 10, and 20. The x-axis has terms 
filed to the pRI, in chronological order of their annotation. 
The 
y-axis shows the cumulative sum of new valid terms found by 
pRI. Figure 6 shows the proportion 
for each N. 

of valid to garbaged 

is illustrated 

terms, 

seeds. 

sets, 

TABLE II. STRICT/LENIENT MANUAL ANNOTATION 

AGREEMENT 

agreed 
disagreed 
agreement 

general 
1584/1661 
269/192 
85%/90% 

no None 
266/343 

82/5 

76%/99% 

were very similar to each other. For instance, 

The two sets of numbers presented 

scores. 

The leniency 

that annotators 

by DASyR. Table II shows 

comes 
often disagreed 
when 

test the quality of the analysis, we observe how often two 
users agree on the terms provided 
their agreement 
are for normal and Lenient agreement. 
from the observation 
two concepts 
between TestCollection and Collection or between 
Challenge and Task. The Lenient agreement 
agreements 
including None. 
general agreement, 
Column no  None removes from consideration 
the None 
annotations, 
between the classes 
both the highest 
it is difficult, even for the users, to clearly 
similar  classes. 

ignores dis­
between these pairs. Column general shows the 

It is in this case that we observe 
indicating 

and the lowest disagreement, 

in order to observe agreement 

between all annotations, 

of interest. 

that 

or disagreement 

distinguish  between 

C. Machine learning 

For the purposes of this evaluation, we focus on the 

for the Measure class. There are 29585 training 

and rv27 mil. candidate 

classification 
vectors 
validation 
2.5% error rate, 77.8% recall,and 

tokens. 
of the radial basis function 

86.4% precision 

Leave-one-out  cross­

kernel SVM estimated 

300 r---r---,--,--,----,r---r--,--,----, 

250 

200 

150 

100 

({) 
E 
2 
:2 
Cii > 

50 

20 ....•.. . 
15 ·_··A··_· 
10 ..  'E!- " 
5 . . .•. . .  
2 ---e---

o L-�_-L_-L_�_L_�_-L_�� 

o  50 1 00  150  200  250 
Terms, ordered 

chronologically 

300  350 400  450 

by their suggestion 
time 
vector space engine 

Fig.s. 

Good suggestions 

made by the semantic 

20 ........ 
15 ._ .. ./1, • •  _. 
10 .. 'E!-" 
5 . ...... 
2 ---e---

20 

15 

10 

(() 
E 
2 
:2 
Cii > 
.f: 
Q; > 0 
:2 
Cii > 

5 

0 

1 
10 
Terms, ordered 

chronologically 

100 

1000 
by their suggestion 
time 

Fig. 6. 

Proportion 

of good suggestions 

B. Annotation 

agreement 

As mentioned 

in Section II-B, the user also has the  option 

the output of 
the result 

for the term 'Recall' .  There are 208 training 
validation 

Leave-one-out 

tokens. 

As an example, we present 

into the ML results by observing 

tree classifier. 

and 4 500 candidate 

to gain insight 
the decision 
of the classifier 
vectors 
estimated 
precision. 
of the term 'Recall' is a Measure is: 
IN(l) A -,that(l) 
A -,the(l) 
-7 None 
IN(l) A that(l) 

Rl 
R2 

7.1% mean absolute error, 94.2% recall, and 93.1% 
The generated 

rule, predicting 

whether an instance 

-7 Measure 

This essentia
followed by the term that. 

lly says that Recall is a Measure if it is not 

D. Use for systematic 

reviews 

Finally, Figure 7 shows the proportion 

of documents 

in 

metrics, 

in 
while 

the background 

(MAP) is predominant

of the number of documents 

one of five popular IR evaluation 

and Recall are obviously frequent, 

each year mentioning 
plotted against 
the year. Precision 
Mean Average Precision 
2000. On viewing this data, we might conjecture 
very low mentions 
Gain (NDCG) metric is related 
collections 
using the NDCG metric), 
the evaluation campaigns 
the NDCG measure. 

that the 
Cummulated 
to the low number of test 

(required 
when 
and decide to look more closely to 
that used these test collections 

with graded relevance judgements 

of the normalized Discounted 

ly used since 

and/or 

For each term highlighted by the system, the annotators 

select from a subset of classes of the IR Ontology 7. To 

IV. CONCLUSIONS AND FUTURE WORK 

7http://ifs.tuwien.ac.

atl�admire/ir_ontolo

gy/ir 

This paper has shown the first steps towards populating 

an IR ontology to support the kind of horizontal 

studies 

we 

594 

2015 13th International 

Conference on Document Analysis and Recognition 

(ICDAR) 

Fig. 7. Distribution 

of five metric mentions 

over years 

a bootstrapping 

an­

Two ML 

30000 

Five experts 

on average. 

annotation 

that annotators 

l Random Indexing 

Future work consists 

in the absence of domain­
therefore, 

to introduce 
added approximately 

we observed 
especially 
We proposed, 

at a speed of 4s/annotation 
were then used to scale from 30k annotations 

now see in the medical domain. Having experimented, 
first, 
with open-end 
consider 
it too time-consuming, 
specific dictionaries. 
method using positiona
notation 
candidates. 
annotations 
approaches 
27 million tokens. 
pruning of the search space and the introduction 
specific chunking 
direction, 
poor performance. 
can now be done on IR research 
very useful insights. 
an important 
The expert annotators 
in using the system to annotate 
there will be a web interface 
observations 

of a domain­
step in the NLP pipeline. Initial steps in this 

l to provide 
The simplicity of the user interface 

Finally, we showed the kind of analysis that 

for our peers to make their own 

using generic chunkers 

primarily in a better 

factor in reducing 

the annotation 

with the potentia

on practices 

also reported 

and trends in IR. 

time per term. 

addictive 

a certain 

[25], have shown relatively 

terms. In the next iteration 

feeling 

is 

to 

[7] A. O'Mara-Eves, 

J. Thomas, J. McNaught, M. Miwa, and S. Ananiadou, 

"Using text mining for study identification 
systematic 
no. 5, 2015. 

review of current 

approaches," 

in systematic 

a 
Reviews, 

Systematic 

vol. 4, 

reviews: 

[8] N. Ferro and G. Silvello, 

From Ad Hoc Retrieval?" 

"CLEF 15th Birthday: What can we Learn 
in Proc.  of 

CLEF, 2014. 

[9] T. Tsikrika, 

A. Garcia Seco de Herrera, 

g the 
"Assessin
2011. 
y Impact of ImageCLEF," in Proc. CLEF Coriference, 

and H. MUller, 

Scholarl

[10] H. de Ribaupierre 

and G. Falquet, 

"A User-centric 

and Retrieve 

Scientific 

Documents," 

Model to Semanti­
in Proc. of ESAIR, 

cally Annotate 
2013. 

[II] D. Schuster, 
A. Hofmeier, 
Document Archiving," in Proc.  of 

K. Muthmann, M. Berger, C. Weidling, K. Aliyev, and 
"Intellix 
for 

-End-User Trained Information 

ICDAR, 2013. 

Extraction 

[12] B. Janssen, 

E. Saund, E. Bier, P. Wall, and M. Sprague, "Receipts2

go: 

the big world of small documents," 

in Proc. of DocEng, 2012. 

[13] E. Medvet, A. Bartoli, 

and G. Davanzo, "A probabilistic 
to 
g," Int. l. Doc. Anal. Recognit., 
vol. 13, 

approach 

understandin

document 

printed 
no. 4, 2011. 

ACKNOWLEDGEMENTS 

This research 

was supported 

by the Austrian 

Science Fund 

(FWF) project  number  P25905-N23  (ADmIRE) 
number I 1094-N23 (MUCKE, under the CHIST-ERA Pro­
gram for Transnationa

Projects). 

and project 

l Research 

Annotation 
Suggestions 
in Proc.  of ACL, 

2014. 

[18] T. Cohen, R. Schvaneveldt, 

[14] D. Maynard, Y. Li, and W. Peters, 
and Ontology Population," 
2008. 
Population, 

"NLP Techniques 

for Term Extraction 

in Proc.  of 

Conf. on Ontology 

Learning & 

[15] C. Lee, P.-M. Ryu, and H. Kim, "Named Entity Recognition Using a 

Modified Pegasos Algorithm," 

in Proc. of CIKM, 20 II. 

[16] D. Gildea and D. Jurafsky, "Automatic 

Labeling of Semantic 

Roles," 

Comput. Linguist., 

vol. 28, no. 3, 2002. 

[17] S. Yimam, R. de Castilho, 

I. Gurevych, and C. Biemann, "Automatic 

and Custom Annotation 

Layers in WebAnno," 

REFERENCES 

inference: 

and D. Widdows, "Reflective 
y 

random 
method for discover
vol. 43, 
Informatics, 

A scalable 

lournal of Biomedical 

indexing and indirect 
of implicit 
no. 2, 2010. [Online]. 
article/pii/S 

connections," 

1532046409001208 

Available: 

http://www.sciencedirect.com/science/ 

[1] D. Gaugh, S. Oliver, 

and J. Thomas, "An Introduction 

to Systematic 

Reviews," 
[2] I. Shemilt, 

Sage, 2012. 

A. Simon, G. Hollands, 

A. O'Mara-Eves, 
mining to reduce impractical 
scoping reviews," 

"Pinpointin

g needles in giant haystacks: 

T. Marteau, D. Ogilvie, 
and 
use of text 

screenin

g workJoad 

in extremely large 

Res. Synth. Methods., 

vol. 13, no. 1218,2013. 

[3] W. R. Hersh and D. Hickam, "How Well Do Physicians 

Use Electronic 

Information 
Systematic 
Association, 

Retrieval 
Review," 
vol. 280, 1998. 

Systems? A framework 
lAMA: The lournal of the American Medical 

for Investigation and 

[4] J. Thomas, "Diffusion 

of information 

in systematic 

review methododol­

[19] [Online]. Available: 
[20] D. Harman, "How effective 
[21] H. Cunningham, D. Maynard, K. Bontcheva, 

http://lucene.apache.or

is suffixing?" lASIS, vol. 42, 1991. 

g/ 

G. Gorrell, 

I. Roberts, 
M. A. Greenwood, 
Text Processing 
http://tin

yurl.com/

with GATE (Version 
gatebook 

V. Tablan, N. Aswani, 
A. Funk, A. Roberts, 
T. Heitz, 
H. Saggion, J. Petrak, Y. Li, and W. Peters, 

D. Damljanovic, 

6), 2011. [Online]. 

Available: 

[22] T. Joachims, 

Learning to Classify Text using Support Vector Machines. 

Kluwer Academic Publishers, 
//www.cs.comell.

eduiPeople/tj/svmtcatbook/ 

2002. [Online]. 

Available: 

http: 

ogy. Why is study selection 
Based Med., vol. I, no. 2, 2013. 

not yet assisted 

by automation?" 

OA Evid. 

[23] R. Quinlan, 

Publishers, 

C4.5: Programs for Machine Learning. Morgan Kaufmann 
San Mateo, CA, 1993. 
F. Piroi, L. Andersson, 
Ontology for Information 

and A. Hanbury, "An Information 

Nanopublications," 

Retrieval 

in 

[24] A. Lipani, 
Retrieval 
Proc. of CLEF, 2014. 

[5] J. Thomas, J. Brunton, 

for Research 
Research 

Synthesis," 
Unit, Institute 
[6] S. Bifft, M. Kalinowski, 

and S. Gaziosi, 
London: EPP!-Centre 
2010. 
of Education, 
F. Ekaputra, 
E. S. Asensio, 

"EPPI-Reviewer 

4.0: Software 

Software, Social Science 

"Building Empirical 
Systematic 

Software 

Engineering Bodies of Knowledge with 

Knowledge Engineering," in Proc.  of 

SEKE, 2014. 

Based Learning," in Natural Language Processing 
Corpora. Springer Netherlands, 

1999, vol. 11. 

Using Very Large 

and D. WinkJer, 

[25] L. Ramshaw and M. Marcus, "Text Chunking Using Transformation­

595 

