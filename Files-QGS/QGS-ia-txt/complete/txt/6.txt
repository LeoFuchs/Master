See discussions, stats, and author proﬁles for this publication at: https://www.researchgate.net/publication/235689154

A new iterative method to reduce workload in systematic review process

Article  in  International Journal of Computational Biology and Drug Design · February 2013
DOI: 10.1504/IJCBDD.2013.052198 · Source: PubMed

CITATIONS
24

2 authors, including:

Siddhartha Jonnalagadda
Conversica
70 PUBLICATIONS   583 CITATIONS   

SEE PROFILE

READS
51

All content following this page was uploaded by Siddhartha Jonnalagadda on 31 July 2015.

The user has requested enhancement of the downloaded ﬁle.

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Int. J. Computational Biology and Drug Design, Vol. 6, Nos. 1/2, 2013 

 

 

5 

A new iterative method to reduce workload in 
systematic review process 

Siddhartha Jonnalagadda* 
Department of Health Sciences Research, 
Mayo Clinic, 
Rochester, Minnesota 55902, USA 
Email: siddhartha@mayo.edu 
*Corresponding author 

Diana Petitti 
Department of Biomedical Informatics, 
Arizona State University, 
Tempe, Phoenix 85281, USA 
Email: diana.petitti@asu.edu 

to 

‘automate’ 

techniques 

language  processing 

Abstract:  High  cost  for  systematic  review  of  biomedical  literature  has 
generated  interest  in  decreasing  overall  workload.  This  can  be  done  by 
applying  natural 
the 
classification of publications that are potentially relevant for a given question. 
Existing solutions need training using a specific supervised machine-learning 
algorithm and feature-extraction system separately for each systematic review. 
We propose a system that only uses the input and feedback of human reviewers 
during  the  course  of  review.  As  the  reviewers  classify  articles,  the  query  is 
modified  using  a  simple  relevance  feedback  algorithm,  and  the  semantically 
closest document to the query is presented. An evaluation of our approach was 
performed using a set of 15 published drug systematic reviews. The number of 
articles that needed to be reviewed was substantially reduced (ranging from 6% 
to 30% for a 95% recall).  

Keywords: systematic review; distributional semantics; information retrieval; 
machine learning. 

Reference  to  this  paper  should  be  made  as  follows:  Jonnalagadda,  S.  and 
Petitti,  D.  (2013)  ‘A  new  iterative  method  to  reduce  workload  in  systematic 
review  process’,  Int.  J.  Computational  Biology  and  Drug  Design,  Vol.  6,  
Nos. 1/2, pp.5–17. 

Biographical  notes:  Siddhartha  Jonnalagadda  currently 
is  a  Research 
Associate at Mayo Clinic-Rochester. He obtained his Bachelors of Technology 
(Honours) in Computer Science and Engineering from the Indian Institute of 
Technology,  Kharagpur  and  his  PhD  degree  in  Biomedical  Informatics  from 
Arizona  State  University.  His  main  research  interest  is  Natural  Language 
Processing (NLP) and focusing on using NLP for clinical knowledge gathering. 

Diana Petitti an Epidemiologist at the Arizona State University in Center for 
Health Information and Research. She served other positions including a field 
Epidemiologist  in  the  Center  for  Disease  Control,  ten  years  as  a  full-time 
faculty  in  the  Department  of  Family  and  Community  Medicine  at  the 
University of California San Francisco, and 19 years as a researcher and then 

Copyright © 2013 Inderscience Enterprises Ltd. 
 
 

 

 

 
 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

6 

 

 

S. Jonnalagadda and D. Petitti 

senior executive with the Kaiser Permanente Medical Care Program. She is one 
of the leading epidemiologic experts on hormones and disease. Her recent work 
has  focused  on  the  applications  of  systematic  review,  meta-analysis  and 
decision  analysis  to  the  formulation  of  health  policy.  She  has  published  two 
books and 200 papers in peer-reviewed journals. 

This  paper  is  a  revised  and  expanded  version  of  a  paper  entitled  ‘A  new 
iterative method to reduce workload in systematic review process’ presented at 
the ‘International Conference on Intelligent Biology and Medicine’, Nashville, 
TN, USA, 22–24 April 2012. 

1 

Introduction 

Systematic  reviews  of  biomedical  literature  are  the  cornerstone  of  the  development  of 
evidence-based  clinical  practice  guidelines.  Systematic  reviews  are  used  not  only  to 
decide the comparative effectiveness of medical treatments, but also as additional input 
on decisions about payment for technologies internationally. 

The steps to conduct a systematic review (Woolf, 1996; Khan et al., 2001; Higgins 

and Sally Green, 2011) are: 
1  To define the review question and develop criteria for including studies 
2  To search for studies addressing the review question 
3  To select studies meeting the criteria for inclusion in the review 
4  To collect data from the studies meeting the criteria for inclusion 
5  To assess the risk of bias in the included studies by appraising them critically 
6  Where appropriate, to analyse the data by undertaking meta-analyses 
7  To address reporting biases 
The results of systematic review are then presented in a report that interprets them and 
then draws conclusions. 

It is nearly impossible to review the full text of all publications identified in step 2 of 
a  well-conducted  review.  Therefore,  step  3  of  this  process  has  historically  involved 
human reviewers reading the abstracts of all publications identified in step 2 to determine 
whether they meet the criteria for inclusion in the review. Reviewers only study the full 
text of a relevant publication if the abstract review suggests that the publication might 
contain data that would address the question posed. 

A well-conducted, comprehensive systematic search for all publications related to a 
topic often yields thousands, or even tens of thousands, of citations to publications. It is 
typical for only a few hundred of the identified publications to be judged as potentially 
relevant based on the abstract review. It is common for only a handful to ultimately be 
found  to  address  the  question  posed  (Upadhyay  et  al.,  2011).  The  abstract  review  to 
determine potential relevance is laborious and is known to be costly (ASHP Foundation, 
2010; DFID, 2010)  

Aphinyanaphongs et al. (2005) proposed the use of machine learning to reduce the 
workload  in  systematic  review.  Over  subsequent  years,  several  other  approaches  to 
replace  manual  (human)  review  of  abstracts  as  a  way  to  reduce  the  effort  have  been 

 
 
 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

A new iterative method to reduce workload 

7 

proposed.  Table  1  describes  these  systems.  All  of  them  employ  supervised  machine 
learning,  with  differences  in  the  machine-learning  algorithm  employed.  Recently, 
Wallace et al. (2010) described the application of active learning, a novel extension of 
supervised machine learning, as an approach to the same problem. Active learning starts 
with  a  small  training  set  and  interactively  obtains  a  more  responsive  training  set.  The 
output of all these systems, however, is a model that encodes the knowledge learned from 
few training examples. The model classifies new documents according to whether they 
are  relevant  for  the  systematic  review  or  not.  This  model  might  not  be  useful  for  a 
systematic  review  of  other  topics.  Further,  most  machine-learning  algorithms  need 
parameter tuning; this has to be done manually by computer engineers. In addition to the 
machine-learning  approaches,  there  are  a  few  approaches  that  use  semantic  processing 
and rules that match question classes for similar tasks (Bray et al., 2008; Fiszman et al., 
2008; Lu et al., 2008; Fiszman et al., 2010). 
Table 1 

Summary of methods proposed for aiding in systematic review 
Year  Machine-learning 
algorithm(s) used 
2005 Naïve Bayes, 

Title 

Adaboost, SVM 

Comments 

First known 
method 

Ref # 

1 

3 

5 

Text categorisation models for high-
quality article retrieval in internal 
medicine 

2  A comparison of citation metrics to 

machine-learning filters for the 
identification of high-quality 
MEDLINE documents 
Reducing workload in systematic 
review preparation using automated 
citation classification 

4  Optimising feature representation for 

automated systematic review work 
prioritisation 
Cross-topic learning for work 
prioritisation in systematic review 
creation and update 

6  A new algorithm for reducing the 
workload of experts in performing 
systematic reviews 
Semi-automated screening of 
biomedical citations for systematic 
reviews 
Toward automating the initial 
screening phase of a systematic review 
Exploiting the systematic review 
protocol for classification of medical 
abstracts 

8 

7 

9 

2006 Support Vector 

Machines (SVM) 

2006 Perceptron based 

voting 

 

 

2008 SVM 

2009 SVM 

Extensive research 
on machine-
learning features 
 

2010 Factorised version of 
Complement Naïve 
Bayes (FCNB) 

 

2010 ensemble of SVMs  Uses active 

learning 

2010 Evolutionary SVM 

2011 FCNB 

 

 

Notes:  Ref #: the citation in References section (1–9) correspond to Aphinyanaphongs 
et  al.  (2005),  Aphinyanaphongs  et  al.  (2006),  Cohen  et  al.  (2006),  Cohen 
(2008),  Cohen  et  al.  (2009),  Matwin  et  al.  (2010),  Wallace  et  al.  (2010), 
Bekhuis and Demner-Fushman (2010) and Frunza et al. (2011); Title: title of 
the paper; Year: the year in which the article is published. 

 
 
 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

8 

 

S. Jonnalagadda and D. Petitti 

It is in this context that we explore whether semantic information can be automatically 
derived  using  distribution  of  the  words  in  Medline  abstracts  as  a  generic  strategy  for 
automating the process of identifying potentially relevant publications from abstracts. We 
also explore the use of an iterative feedback system that eliminates the need for creating a 
separate training set in an online learning kind of set-up. Such a system could be readily 
used  for  any  systematic  review,  even  if  the  reduction  in  workload  is  not  as  high  as  a 
supervised machine-learning system. 

2  Methods 

Our approach to reducing the workload of systematic review and eliminating the need for 
the  systematic  reviewers  to  interact  with  informatics  professionals  separately  for  each 
review  topic  is  based  on  the  use  of  distributional  semantics  (semantics  empirically 
derived  from  text)  (Cohen  and  Widdows,  2009;  Jonnalagadda  et  al.,  2012).  Figure  1 
depicts the architecture of the system. Abstracts are first uploaded to the system and then 
a semantic model of the terms is created during the preparation or preprocessing phase. It 
is  also  possible  to  use  a  previously  created  semantic  model  of  terms  using  Medline 
abstracts  to  avoid  preprocessing.  A  randomly  selected  document  is  presented  to  the 
reviewer,  who  annotates  and  classifies  the  document  as  potentially  relevant  or  not 
relevant. The semantic model is then used as the basis for presenting the next document 
to the reviewer based on the similarity of the document to the terms in the document and 
to the document just classified. This document is annotated and classified as relevant or 
not  relevant.  The  feedback  from  the  relevance  classification  by  experts  and  is  used  to 
present documents to the reviewer that are increasingly likely to be relevant, based on 
information from the documents that have been classified as relevant or not relevant to 
that  point.  The  reviewer  can  elect  to  end  the  process  of  classifying  documents  at  any 
point, recognising that stopping before reviewing all documents involves a trade-off of 
lower recall for reduced workload. 

Figure 1  Architecture  of  the  system:  a  semantic  model  is  built  for  the  terms  present  in  the 
documents  that  need  to  be  systematically  reviewed.  Using  the  initial  query  and  our 
relevance feedback algorithm that uses the expert review for the documents annotated 
for relevance so far, the next document that is most likely to be eligible is presented 
(see online version for colours) 

 

 
 
 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

A new iterative method to reduce workload 

9 

2.1  Semantic search 

In  prior  text-mining  applications  to  systematic  review,  the  documents  are  classified 
dichotomously  as  relevant  (included)  or  not  relevant  (excluded).  Our  approach  uses  a 
semantic  vector  model  of  the  terms  present  in  the  abstracts  to  rank  the  documents  in 
order of their potential relevance. The ranking of documents is an important feature that 
distinguishes  the  approach  we  describe  from  prior  approaches.  The  semantic  vector 
model of terms, also referred to as ‘wordspace’, is learned using the sliding windows of 
the  words  in  the  Medline  abstracts.  All  Medline  citations  of  the  2009  baseline  (NLM, 
2010) that have abstracts (~9 million) are used for creating the term vectors so that the 
terms  are  more  accurately  represented  in  the  wordspace.  Using  the  cosine  distances 
between vector representations of the modified query and the documents, the next most 
relevant document is calculated iteratively. 

In a typical vector representation of terms and documents, each term is considered 
completely  independent.  Thus,  a  search  on  ‘diseased’  and  ‘sick’  might  result  in  a 
completely  different  document  ranking  as  measured  by  the  distance  between  the 
document  vector  and  the  term  vector.  Recent  research  (Cohen  and  Widdows,  2009) 
suggests that the semantic representations using distributional information of the terms, 
such as Hyperspace Analogue to Language (HAL) (Lund and Burgess, 1996) and Latent 
Semantic Analysis (LSA) (Landauer and Dumais, 1997), yield better results. We reduce 
the  dimensionality  of  vectors  using  random  indexing  and  construct  a  vector  space  of 
terms using the directional model (simplified version of HAL). 

2.2  Random indexing 

First,  we  introduce  random  indexing.  Geometric  models  of  distributional  semantics 
represent  each  term  as  a  vector  in  high-dimensional  space.  Distributional  semantic 
models,  constructed  based  on  millions  of  documents  and/or  millions  of  terms  such  as 
those  represented  in  Medline  abstracts,  would  be  unmanageable  by  size.  The  models 
approaching corpora of this magnitude tend to reduce dimensionality first. Traditional, 
dimensionality reduction techniques, such as Singular Value Decomposition (SVD), are 
computationally  expensive  (the  commonly  utilised  algorithm  for  SVD  is  cubic  in 
complexity)  (Trefethen  and  Bau,  1997).  Recently,  Random  Indexing  (Kanerva  et  al., 
2000) emerged as a promising alternative to the use of SVD for the dimension reduction 
step  in  the  generation  of  term-by-context  vectors.  Random  Indexing  and  other  
similar  methods  are  motivated  by  the  Johnson–Lindenstrauss  Lemma  (Johnson  and 
Lindenstrauss, 1984) that states that the distance between points in a vector space will be 
approximately  preserved  if  they  are  projected  into  a  reduced-dimensional  subspace  of 
sufficient dimensionality. Random Indexing scales at a rate that is linear to the size of the 
data, since the term-document or term-term matrix need not be stored in memory. This is 
accomplished  by  assigning  to  each  document  (in  term-document  models)  or  term  
(in sliding-window models) a sparse high-dimensional (on the order of 1000) elemental 
vector, a vector comprising of mostly zero elements with a small number (on the order  
of 10)  set  to  either  +1 or –1.  These non-zero  elements  are  determined  at  random,  and 
because  of  the  sparseness  of  the  vectors,  the  resulting  vectors  are  highly  likely  to  be 
orthogonal or close-to-orthogonal to one another.  

 
 

 
 
 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

10 

 

S. Jonnalagadda and D. Petitti 

We  use  the  Semantic  Vectors  package  (Cohen  and  Widdows,  2010)  to  create 
elemental vectors for the terms  in Medline abstracts using random indexing. Based on 
our previous experiments (Jonnalagadda et al., 2012), which revealed that using 2000-
dimensional  vectors  and  five  seeds  (number  of  +1s  and  –1s  in  the  vector)  are  most 
optimal, we create the elemental vectors for all the terms in each of the 9 million Medline 
abstracts. Among different types of distributional models implemented in the Semantic 
Vectors  package  (Cohen  and  Widdows,  2010)  (Basic,  Positional,  Directional,  and 
Positional + Basic), the Directional model was shown to optimally assign similar vectors 
(in direction) to terms appearing in similar context (Jonnalagadda et al., 2012). 

2.3  Directional model 

The algorithm uses a sliding window that is moved through the text corpus to generate a 
term-term matrix, T, where T[i, j] is the number of times the word representing the j-th 
column appears near the word representing the i-th column. Two words are in the vicinity 
of each other if, and only if, the number of words separating them is less than an integer 
parameter  known  as  the  sliding-window  radius.  The  directional  model  also  takes  into 
account  the  direction  in  which  a  word  occurs  with  respect  to  another  by  having  two 
columns for each word, with one column representing the number of occurrences to the 
left and the other column representing the number of occurrences to the right. 

2.4  Document ranking  

The documents and the query were mapped to the vector space as follows: 

(
s C

)

=

T

∗∑
[ ]
n i

i

1
=

(
[ ]
s t i

)

,

 

(1) 

where C is a collection of terms such as a document or a term, s() is the unit semantic 
vector, t[i] is the i-th term, n[i] is the number of times t[i] occurs in C, and || || is the norm 
operator. 

The cosines of the document vectors with respect to the query vector are measured. 

The most relevant document d for the query q among a set of documents D is given by: 

d

= ∈

d D x D

∀ ∈

(

{

,cos

(

(
s x

)

,

(
s q

)

)

≤

cos

(

(
(
s d s q

)

,

)

)

}
)

 

(2) 

2.5  Relevance feedback  

Our approach uses feedback from the reviewers (as shown in Figure 1) as they review the 
abstracts presented to them by the system to modify. This is ‘relevance feedback’. The 
incorporation  of  feedback  is  the  second  feature  that  distinguishes  our  approach  from 
those described previously, although Wallace et al.’s (2010) prototype system also uses 
relevance feedback through active learning to create a training set. 

The  system  first  asks  the  reviewer  to  describe  the  study  using  salient  words  or  a 
simple query. The initial vector was constructed by the initial query terms set (Q0) given 
by the reviewer. If the reviewer decides not to give an initial query, Q0 is an empty set. 

 
 

 
 
 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

A new iterative method to reduce workload 

The set of query terms after m documents (Qm) were reviewed is given by: 

Q
m

=

(

Q P
m

∪

0

)

−

N

,

 

m

11 

(3) 

where Nm are the terms that appear in the documents reviewed as not relevant so far, Pm 
are the terms that appear in the documents reviewed as relevant so far, and Qm is used to 
create the query vector after reviewing m documents by adding the vectors for each term 
in the set. 

As  evident  from  equation  (3),  the  query  at  any  stage  depends  on  the  terms  in  the 
documents  reviewed  so  far.  The  most  relevant  document  to  a  query  in  the  remaining 
documents  is  selected  using  equation  (2). After  sufficient  experience  with  a  variety  of 
topics, a cut-off criteria could be decided by the users (or suggested by the system) based 
on  the  percentage  of  articles  reviewed,  the  cosine  similarity  value  of  the  document 
presented as most appropriate, and the number of successive irrelevant articles presented 
by the system. 

2.6  Description of the systematic reviews used for the evaluation 

Our  system  needs  no  training  set  data  for  development.  However,  we  need  a  set  of 
annotated documents so that the annotation could be used to simulate the reviewer user 
of the system. We evaluated our system using information from 15 systematic reviews of 
drug  classes  that  Cohen  et  al.  (2006)  used  to  train  their  supervised  machine-learning 
system. The different categories provided by Cohen et al. are merged to create a binary 
classification  of  relevant  or  not  relevant  documents.  Although  the  abstracts  are  not 
representative, given the high percentages of inclusion, this is the only publicly available 
collection.  For  these  15  systematic  reviews,  Cohen  et  al.  have  made  available  (see 
http://medir.ohsu.edu/~cohenaa/systematic-drug-class-review-data.html) the PubMed IDs 
of the abstracts that they manually reviewed, along with the corresponding classification 
of each abstract. Table 2 presents the 15 drug review topics along with the number of 
abstracts  reviewed,  the  number  of  abstracts  included,  and  the  number  of  abstracts 
excluded by Cohen et al. in their review. 
Table 2 

Drug class reviews used for validation of the method 

Drug Class 

ACE inhibitors 
Attention deficit  
hyperactivity disorder 
Antihistamines 
Atypical antipsychotics 
Beta blockers 
Calcium channel blockers 
Estrogens 
Non-steroidal antiflammatory 
drugs (NSAIDS) 
 

UMLS 

C0003015 

C1263846 

C0019590 
C0040615 
C0001645 
C0006684 
C0202006 

C0003211 

Total 

abstracts 

Included 
abstracts 

Excluded 
abstracts 

2544 

851 

310 
1120 
2072 
1218 
368 

393 

183 

84 

92 
363 
302 
279 
80 

88 

2361 

767 

218 
757 
1770 
939 
288 

305 

 
 
 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

12 

 

S. Jonnalagadda and D. Petitti 

Table 2 

Drug class reviews used for validation of the method (continued) 

Drug Class 

UMLS 

Total  
abstracts 

Included  
abstracts 

Excluded 
abstracts 

C0029104 
C0571635 
C0358591 
C0037250 
C0360704 
C1567966 
C0042024 

Opioids 
Oral hypoglycemics 
Proton pump inhibitors 
Skeletal muscle relaxants 
Statins 
Triptans 
Urinary incontinence 
Notes:  Each  row  in  the  table  corresponds  to  a  class  of  drugs.  Certain  rows  that  are 
named  after  a  medical  condition,  such  as  ‘Attention  deficit  hyperactivity 
disorder’,  correspond  to  the  class  of  drugs  that  treat  the  condition.  For  more 
detail about the drug class, UMLS (Aronson, 2001) codes are assigned (bold 
for medical conditions and normal for actual drug classes). 

1915 
503 
1333 
1643 
3465 
671 
327 

48 
139 
238 
34 
173 
218 
78 

1867 
364 
1095 
1609 
3292 
453 
249 

In systematic reviews done with the aim of developing practice guidelines and clinical 
policy, all abstracts are first studied to identify and eliminate the clearly irrelevant ones 
from further review. The full texts of the possibly eligible documents are then reviewed 
to identify those that are actually relevant. In our evaluation, we use only abstracts, not 
full text, to make our final determination of relevance, because some full texts were not 
freely available. The work saved over sampling at X%, or WSS@X%, also defined by 
Cohen et al. (2006), will be used for evaluation: 

WSS

@ %

X

=

TN FN

+
N

1
− +

X
100

,

 

(4) 

where TN is the number of true negatives identified by the system, FN is the number of 
false negatives identified by the system, N is the total number of documents in the test 
set, and X is the recall rate.  

3  Results 

We  assessed  the  performance  of  our  relevance  feedback-based  system  in  terms  of 
reduction in workload based on Cohen’s 15 drug class reviews. Since the key questions 
these  reviews  try  to  address  are  unknown  and  the  objective  is  to  test  the  system,  the 
initial  queries  are  not  set,  although  highly  precise  queries  would  result  in  better 
performance. The query vector becomes more and more relevant to the task as we use 
relevance feedback. Figure 2 shows how recall changes as  more articles are reviewed. 
Considering  both  workload  and  recall,  an  ideal  system  for  selection  of  abstracts  for 
human  review  of  the  full  text  of  articles  would  have  100%  recall  when  the  number  
of  abstracts  presented  to  the  expert  reviewer  by  the  systems  is  exactly  equal  to  the 
number  classified  as  potentially  relevant  in  the  ‘gold  standard’  (manually  reviewed) 
abstracts. 

 
 
 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

A new iterative method to reduce workload 

13 

Figure 2  Performance of the system on 15 drug systematic reviews. The X-axis represents the 
percentage of abstracts reviewed and the Y-axis represents the recall. Plots are shown 
for all the 15 drug reviews (see online version for colours) 

 

Figure  2  shows  how  recall  changes  as  more  articles  are  reviewed.  Recall  at  a  given 
proportion of articles reviewed varies substantially by topic. At an arbitrary threshold of 
a  recall  of  90%,  the  percentage  of  all  abstracts  reviewed  is  as  low  as  43%  (attention 
deficit hyperactivity disorder) and as high as 95% (opiods). To assure attainment of 95% 
recall  across  all  15  topics,  as  might  be  required  when  using  the  system  as  a  generic 
approach to selection of abstracts for human review, it would be necessary to review 95% 
of all abstracts. If relevance feedback was not useful, the curves would have been straight 
lines, reflecting presentation of articles in random order. Figure 2 shows that relevance 
feedback  leads  to  a  large  initial  increase  in  recall.  As  more  and  more  information  is 
added, the increase in recall becomes more gradual.  

Figure 3 shows the percentages of work saved in review over sampling at 95% recall 
comparing  our  system  with  results  presented  in  the  work  of  Cohen  et  al.  for  the 
supervised  machine-learning  system.  Cohen  et  al.  (2006)  used  a  5 × 2  cross  validation 
(half of each corpus for training and the other half for testing) on a supervised machine-
learning  system.  Our  results  are  based  on  the  entire  collection  of  documents  for  each 
review.  Considering  workload  reduction,  our  results  are  broadly  comparable  to  those  
of  Cohen  et  al.  (as  shown  in  Figure  3).  Estimated  workload  reductions  at  95%  recall 
range  from  6%  to  30%  for  our  system  and  from  0%  to  68%  for  Cohen’s  workload; 
median-estimated  workload  reduction  at  95%  recall  is  13%  for  our  system  and  18%  
for  Cohen’s  system.  Using  our  system,  we  estimate  a  reduction  in  workload  for  all  
15 reviews, whereas Cohen et al.’s system suggested a reduction in workload for 13 of 
the  15  reviews.  Our  system  had  a  better  performance  than  Cohen  et  al.’s  did  for  five 
reviews. 

Our  findings  provide  strong  support  for  the  conduct  of  further  research  to  create 
unsupervised  systems  to  reduce  workload  in  a  systematic  review  process.  Unlike 
supervised systems, they do not add additional workload of creating a training set or of 
building a trained model that might involve interacting with computer engineers. 

 

 
 
 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

14 

 

S. Jonnalagadda and D. Petitti 

Figure 3  Comparison of our system with Cohen et al.’s (2006). Jonnalagadda WSS@95% is the 
label for the respective percentage of work saved over a sampling of 95% recall for the 
current system. Cohen WSS@95% is the label for the respective percentage of work 
saved  over  a  sampling  of  95%  recall  for  Cohen  et  al.’s  system  (see  online  version  
for colours) 

4  Discussion 

 

We described a system that reduces the workload of systematic review based on the use 
of semantic features of the document to identify potentially relevant documents. We have 
coupled  this  with  feedback  about  relevance  to  the  system  based  on  classification  by 
experts that results in documents more likely to be relevant when presented to the expert 
earlier. 

Semantic  features  in  the  form  of  manually  assigned  MeSH  terms  have  been 
previously  used  by  Cohen  in  a  similar  attempt  to  reduce  the  workload  of  systematic 
review (Cohen, 2008). Our system is different from Cohen’s in that the semantic features 
are created automatically. The system uses a directional model for creating the semantic 
vectors, which are created for terms that are paradigmatically related. If two terms can be 
substituted  for  each  other  in  a  sentence  (i.e.  they  occur  in  similar  local  contexts 
throughout the corpus), they are said to be in a paradigmatic relationship. Examples of 
terms  in  a  paradigmatic  relationship  are  p53  (gene)  and  gata1  (gene);  AD  and  SDAT 
(synonyms);  and  poliomyelitis  and  polio  (synonyms).  The  directional  model  approach 
enables semantic search, where the user needs not enter all the synonyms for a particular 
concept to get all the relevant documents. 

Using  a  traditional  supervised  machine-learning  approach,  it  is  possible  that  a 
document will not be classified as relevant because it uses different words or n-grams to 
convey  a  concept.  Since  documents  are  represented  as  a  complex  vector  or  logical 
combination  of  various  features;  in  traditional  approaches,  it  is  not  easy  for  users  to 
modify  the  criteria  for  document  selection.  In  the  proposed  system,  the  distributional 
semantic model assigns nearby vectors to contextually similar words. Therefore, even if 
an important (key) word is paraphrased or replaced by a similar word in an unannotated 

 
 
 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

A new iterative method to reduce workload 

15 

document, it is likely to be ranked high. In addition, the dynamically changing query set, 
which decides which document will be presented next, could be easily modified at any 
stage by removing or adding terms to the query set. In this way, the possibility of not 
finding  documents  that  use  different  words  could  be  further  minimised  by  allowing 
active participation of the users in defining the terms in the query set. 

In traditional supervised machine-learning approach, externally supplied instances are 
used  to  create  a  model  that  classifies  future  instances  (Kotsiantis,  2007).  In  our 
framework, no  instances  are supplied  initially  to  the  system,  and  predictions  are  made 
using  the  classification  of  test  set  instances.  Our  approach  has  similarities  with  active 
learning,  but  it  is  designed  to  be  easier  to  adapt.  The  application  of  active  learning  to 
assist systematic review is also novel (Wallace et al., 2010). Future work might involve 
comparing  the  performance  of  these  two  methods  and  perhaps  integrating  them.  A 
second  application  of  our  system  would  be  to  obtain  a  balanced  training  set  for  a 
traditional-supervised  system.  The  first  1000  (or  so)  documents  reviewed  using  our 
system  would  have  higher  number  of  relevant  documents  than  the  same  number  of 
documents selected at random. 

Across the 15 topics we examined, our system was not able to assure a high rate of 
recall  (90%–95%)  with  a  substantial  reduction  (40%)  in  workload  reliably.  The 
acceptability  of  both  our  system  and  other  systems  that  attempt  to  substitute  modern 
informatics  approaches  for  human  labour  is  not  yet  known.  If  those  who  rely  on 
systematic review to develop guidelines and policy demand 100% recall and informatics 
approaches such as ours are not able to guarantee 100% recall, the approaches may be 
doomed. Applications of the approaches to assure more frequent updating of systematic 
reviews  might  be  more  acceptable  than  use  for  de  novo  review.  However,  our  system 
provides  a  framework  that  only  complements  the  manual  review  process  by  passively 
using feedback from the reviewer to determine the order of review. Further empiric work 
with policymakers should accompany the development of approaches like ours. 

5  Conclusion 

We  proposed  the  use  of  distributional  semantics  and  user  feedback  as  an  approach  to 
reduce  workload  in  systematic  review.  The  system  might  be  immediately  useful  as  an 
enhancement  to  existing  traditional  supervised  learning  systems  by  creating  balanced 
training sets. Even though the system currently does not use sophisticated features such 
as  n-grams,  MeSH  terms  and  UMLS  identifiers  and  does  not  have  a  separate  training 
phase; its performance is comparable to a well-known existing system that also attempts 
to  reduce  workload  of  systematic  review  (albeit  by  applying  supervised  machine 
learning).  Future  work  would  involve  integrating  the  system  with  an  active  learning 
system and incorporating the above features. 

Acknowledgements 

The  core  code  to  create  distributional  semantic  wordspaces  is  available  as  part  of  the 
Semantic  Vectors  package  (see  http://code.google.com/p/semanticvectors).  We  would 
also like to thank Aaron Cohen for making available the 15 systematic review data. This 
work is partly supported by the National Library of Medicine grant (1K99LM011389 – 
PI:Jonnalagadda). 

 
 
 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

16 

 

S. Jonnalagadda and D. Petitti 

References 
Aphinyanaphongs, Y., Tsamardinos, I., Statnikov, A., Hardin, D. and Aliferis, C.F. (2005) ‘Text 
categorization  models  for  high-quality  article  retrieval  in  internal  medicine’, Journal  of  the 
American Medical Informatics Association: JAMIA, Vol. 12, No. 2, pp.207–216. 

Aphinyanaphongs, Y., Statnikov, A. and Aliferis, C.F. (2006) ‘A comparison of citation metrics to 
machine learning filters for the identification of high quality MEDLINE documents’, Journal 
of the American Medical Informatics Association: JAMIA, Vol. 13, No. 4, pp.446–455. 

Aronson,  A.R.  (2001)  ‘Effective  mapping  of  biomedical  text  to  the  UMLS  Metathesaurus:  the 

MetaMap program’, American Medical Informatics Association (AMIA), pp.17–21. 

ASHP  Foundation  (2010)  Demonstrating  Pharmacists’  Value:  A  Systematic  Evidence  Review 
Request  for  Proposals.  Available  online  at:  http://www.ashpfoundation.org/MainMenu 
Categories/ResearchResourceCenter/FundingOpportunities/DemonstratingPharmacistsValueA
SystematicEvidenceReviewRequestforProposals.aspx (accessed on 17 May 2011). 

Bekhuis,  T.  and  Demner-Fushman,  D.  (2010)  ‘Towards  automating  the  initial  screening  phase  
of  a  systematic  review’,  Studies  in  Health  Technology  and  Informatics,  Vol.  160,  No.  1, 
pp.146–150. 

Bray,  B.E.,  Fiszman,  M.,  Shin,  D.  and  Rindflesch,  T.C.  (2008)  ‘Using  semantic  predications  to 
characterize  the  clinical  cardiovascular  literature’,  AMIA  Annual  Symposium  Proceedings/ 
AMIA Symposium, p.887. 

Cohen, A.M., Hersh, W.R., Peterson, K. and Yen, P-Y. (2006) ‘Reducing workload in systematic 
review preparation using automated citation classification’, Journal of the American Medical 
Informatics Association: JAMIA, Vol. 13, No. 2, pp.206–219. 

Cohen,  A.M.  (2008)  ‘Optimizing  feature  representation  for  automated  systematic  review  work 

prioritization’, AMIA Annual Symposium Proceedings/AMIA Symposium, pp.121–125. 

Cohen, A.M., Ambert, K. and McDonagh, M. (2009) ‘Cross-topic learning for work prioritization 
in  systematic  review  creation  and  update’,  Journal  of  the  American  Medical  Informatics 
Association: JAMIA, Vol. 16, No. 5, pp.690–704. 

Cohen, T. and Widdows, D. (2009) ‘Empirical distributional semantics: methods and biomedical 

applications’, Journal of Biomedical Informatics, Vol. 42, No. 2, pp.390–405. 

DFID  (2010)  DFID  Systematic  Review  Pilot.  Available  online  at:  http://www.dfid.gov.uk/ 

r4d/PDF/Publications/DFID_Systematic_Review_FAQ.pdf (accessed on 17 May 2011). 

Fiszman, M., Bray, B.E., Shin, D., Kilicoglu, H., Bennett, G.C., Bodenreider, O. and Rindflesch, 
T.C.  (2010)  ‘Combining  relevance  assignment  with  quality  of  the  evidence  to  support 
guideline  development’,  Studies  in  Health  Technology  and  Informatics,  Vol.  160,  No.  1, 
pp.709–713. 

Fiszman, M., Ortiz, E., Bray, B.E. and Rindflesch, T.C. (2008) ‘Semantic processing to support 
clinical  guideline  development’,  AMIA  Annual  Symposium  Proceedings/AMIA  Symposium, 
pp.187–191. 

Frunza, O., Inkpen, D., Martin, S., Klement, W. and O’Blenis, P. (2011) ‘Exploiting the systematic 
review  protocol  for  classification  of  medical  abstracts’,  Artificial  Intelligence  in  Medicine, 
Vol. 51, No. 1, pp.17–25. 

Higgins,  J.P.T.  and  Sally  Green,  P.  (2011)  Cochrane  Handbook  for  Systematic  Reviews  of 

Interventions, John Wiley & Sons, Chichester, UK. 

Johnson,  W.B.  and  Lindenstrauss,  J.  (1984)  ‘Extensions  of  Lipschitz  mappings  into  a  Hilbert 

space’, Contemporary Mathematics, Vol. 26. 

Jonnalagadda,  S.,  Cohen,  T.,  Wu,  S.  and  Gonzalez,  G.  (2012)  ‘Enhancing  clinical  concept 
extraction with distributional semantics’, Journal of Biomedical Informatics, Vol. 45, No. 1, 
pp.129–140. 

Kanerva, P., Kristofersson, J. and Holst, A. (2000) ‘Random indexing of text samples for latent 

semantic analysis’, 22nd Annual Conference of the Cognitive Science Society. 

 
 
 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

View publication stats
View publication stats

 

 

 

 

 

A new iterative method to reduce workload 

17 

Khan, K.S., ter Riet, G., Glanville, J., Sowden, A.J. and Kleijnen, J. (2001) Undertaking Systematic 
Reviews of Research on Effectiveness: CRD’s Guidance for Carrying out or Commissioning 
Reviews, NHS Centre for Reviews and Dissemination. 

Kotsiantis,  S.B.  (2007)  ‘Supervised  machine  learning:  a  review  of  classification  techniques’, 
Proceedings  of  the  2007  conference  on  Emerging  Artificial  Intelligence  Applications  in 
Computer Engineering: Real Word AI Systems with Applications in eHealth, HCI, Information 
Retrieval and Pervasive Technologies, IOS Press, Amsterdam, pp.2–24. 

Landauer,  T.K.  and  Dumais,  S.T.  (1997)  ‘A  solution  to  Plato’s  problem:  the  latent  semantic 
analysis  theory  of  acquisition,  induction,  and  representation  of  knowledge’,  Psychological 
review, Vol. 104, No. 2, pp.211–240. 

Lu, Z., Kim, W. and Wilbur, W.J. (2008) ‘Evaluating relevance ranking strategies for MEDLINE 

retrieval’, AMIA Annual Symposium Proceedings/AMIA Symposium, p.439. 

Lund,  K.  and  Burgess,  C.  (1996)  ‘Hyperspace  analog  to  language  (HAL):  a  general  model  of 

semantic representation’, Language and Cognitive Processes, Vol. 30, p.265. 

Matwin, S., Kouznetsov, A., Inkpen, D., Frunza, O. and O’Blenis, P. (2010) ‘A new algorithm for 
reducing the workload of experts in performing systematic reviews’, Journal of the American 
Medical Informatics Association: JAMIA, Vol. 17, No. 4, pp.446–453. 

NLM  (2010)  2009  MEDLINE®/PubMed®  Baseline  Statistics.  Available  online  at:  http://www. 

nlm.nih.gov/archive//20100419/bsd/licensee/2009_stats/2009_LO.html 

Trefethen, L.N. and Bau, D. (1997) Numerical Linear Algebra, Society for Industrial Mathematics. 
Upadhyay, A., Earley, A., Haynes, S.M. and Uhlig, K. (2011) ‘Systematic review: blood pressure 
target  in  chronic  kidney  disease  and  proteinuria  as  an  effect  modifier’,  Annals  of  Internal 
Medicine, Vol. 154, No. 8, pp.541–548. 

Wallace, B.C., Trikalinos, T.A., Lau, J., Brodley, C. and Schmid, C.H. (2010) ‘Semi-automated 
screening of biomedical citations for systematic reviews’, BMC Bioinformatics, Vol. 11, p.55. 
Woolf, S.H. (1996) ‘Manual for conducting systematic reviews’, Agency for Health Care Policy 

and Research. 

 
 
 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

