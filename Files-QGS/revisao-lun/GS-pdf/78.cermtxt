Physical Authentication
of Control Systems
Designing watermarke
D control inputs
to Detect counterfeit sensor outputs
Ynli M,o Sean Wreakod
Y, and Bunro Snipoli
Cputation, and control into physical spaces with
yberphysical systems (CPSs) refer to the embedding
of widespread sensing, networking, comthe
goal of making them safer, more efcfiient,
and reliable. Driven by the miniaturization and
integration of sensing, communication, and computation
in cost-efcfiient devices, CPSs are bound to transform industries
such as aerospace, transportation, built environDigital
Object Identifier 10.1109/MCS.2014.2364724
Date of publication: 19 January 2015
1066-033X/15©2015ei
ments, energy, health care, and manufacturing, to name a
few. This great opportunity, unfortunately, is matched by
even greater challenges. Taming the complexity of design
and analysis of these systems poses a fundamental problem
as a new paradigm is needed to bridge various scienticfi
domains, which, through the years, have developed
signicfiantly different formalisms and methodologies. In
addition, while the use of dedicated communication networks
has so far sheltered systems from the outside world,
use of off-the-shelf networking and computing, combined
with the unattended operation of a plethora of devices,
FEBRUARY 2015 « IEEE CONTROL SYSTEMS MAGAZINE 93
The Stuxnet Attack
Srichment facilities in Iran, reportedly causing damage to suspended during the second version of Stuxnet. In the first vertuxnet
was a complex malware that infected uranium en- legitimate control logic reading the frequency converter was
approximately 1000 centrifuges at these plants [S1]. From a sion, legitimate controller code continued to run but was isolated
cyberperspective, Stuxnet was sophisticated, exploiting four from the true dynamics of the system. Stuxnet intercepted all
previously unknown or zero-day Microsoft Windows vulnerabili- input and output signals. To cause damage, Stuxnet implementties.
Additionally, Stuxnet used the first known programmable ed control logic causing pressure in centrifuges to increase. To
logic controller (PLC) root kit, which is malicious stealthy code prevent detection, Stuxnet also delivered previously recorded
injected into controllers that can hide its own existence [S2]. measurements to the SCADA system, performing a replay atMoreover,
to reduce the chance of being detected, Stuxnet was tack. Replay attacks are commonly considered in information
signed by two stolen certificates from chip manufacturers Re- security where adversaries masquerade as a trusted party by
altek and JMicron [S3]. The attack itself was targeted with the repeating a valid data transmission they have intercepted [S5].
worm designed for local distribution, mainly through USB sticks In information security, replay attacks are thwarted by ensuring
or local networks. While Stuxnet eventually reached 100,000 a message is current through session tokens or time stamps or
computer systems, only specific controllers were targeted by by authenticating the sender. Since the Stuxnet attack is linked
the malware [S4]. In particular, Stuxnet targeted PLCs manu- to a physical process, as opposed to residing strictly in the cyber
factured by Siemens and installed malicious code only after realm, this article proposes physical watermarking as a systemchecking
that model numbers, configuration details, and pro- theoretic method to verify the “freshness” of sensor outputs.
gram code matched its target. In the first version of Stuxnet,
gas pressure of uranium hexafluoride in the centrifuge was REfERENCES
increased, which can cause stress to rotors. In a second ver- [S1] D. P. Fidler, “Was Stuxnet an act of war? Decoding a cyberattack,”
sion, the worm varied rotational speeds. When operating near [ISEE2]ESS.eKcaurrn. oPursivkaocsy,,“vSotlu.x9n, entow.4o,r mppi.m5p6a-c5t9o,n20in1d1u.strial cyber-physical
certain critical speeds or harmonics, this attack can cause the system security,” in Proc. 37th Annu. Conf. IEEE Industrial Electronics
rotor to vibrate or even break [1]. Society, Melbourne, Australia, 2011, pp. 4490-4494.
Another question to consider is how Stuxnet evaded detec- [NSo3t]e]T,”. IEME.ECNheetnw, o“rSkt,uvxonl.e2t,4,thneo.r6e,aplps.ta2r-t3o,f20c1y0b.er warfare? [Editor's
tion from verification systems, which measure the system state. [S4] R. Langner, “Stuxnet: Dissecting a cyberwarfare weapon,” IEEE
According to [1], rotor speed is not a controlled variable and is Secur. Privacy, vol. 9, no. 3, pp. 49-51, 2011.
unlikely to be measured. Furthermore, a monitoring application [cSo5ls]],P”.inSyPvreorcs.oCno, m“AputatexornSoemcuyriotyf rFeopulanydaattitoancsksW[ocrrkysphtoogpraVpIIh,icFrparnoctoo-would
have seen rotor speed values prior to an attack since nia, NH, 1994, pp. 187-191.
provides several opportunities for malicious entities to inject
attacks on CPSs. A wide variety of motivations exists
for launching an attack on CPSs, ranging from economic
reasons, such as obtaining a financial gain, all the way to
terrorism, for instance, threatening an entire population
by manipulating life-critical resources. Any attack on safety-critical
CPSs may signicfiantly hamper the economy
and lead to the loss of human lives. While the threat of attacks
on CPSs tends to be underplayed at times, the Stuxnet
worm provided a clear example of the possible future
to come. This malware, targeting a uranium enriching
facility in Iran, managed to reach the supervisory control
and data acquisition (SCADA) system controlling the centrifuges
used in the enrichment process. Stuxnet modified
the control system, increasing pressure in the centrifuges
in a first version of the worm and spinning centrifuges in
an erratic fashion in a second version. As a result, Stuxnet
caused significant damage to the plant [1]. For details, see
“The Stuxnet Attack.”
This article proposes a control-theoretic method, called
physical watermarking, to authenticate the correct operation
of a control system. Authentication in CPSs requires
that the system operator verify the identity of a component
94 IEEE CONTROL SYSTEMS MAGAZINE » FEBRUARY 2015
not only in the cyberworld but also within the framework
of the physical dynamics of the system. While tools exist in
cryptography to perform authentication, historical data
show that attackers often manage to break these security
mechanisms. Moreover, such tools are ineffective against
physical attacks on the system or insiders who are usually
authenticated users. There arises the need to expand and
complement the existing set of tools to improve the resilience
of CPSs. The presence of a dynamical system, while
presenting new challenges, offers new opportunities to
improve detection and resilience. The existence of accurate
mathematical models describing the underlying physical
phenomena enables the prediction of future behavior and,
more importantly, unforeseen deviations from it. The ability
to recognize irregularities in the dynamics of a system
enables a principled approach to intrusion detection and
resilient design. The concept of physical watermarking
emerges in this context. Its utility lies in its ability to allow
physical authentication of physical components. By injecting
a known noisy input to a physical system, it is expected
that the effect of this input can be found in the measurement
of the true output, due to the system dynamics. As
such, if an attacker is unaware of this physical watermark,
the system cannot be adequately emulated because the
attacker is unable to consistently generate the component of
the output associated with this known noisy input. Consequently,
the watermark acts as a physical nonce, forcing an
attacker to generate outputs unique to the given inputs at a
chosen time. The next section further examines existing
work and techniques related to CPS security.
RELATEd W ORk
Fault detection and isolation methods have been extensively
studied over the past decades to detect anomalies in
a system by measuring the discrepancies between the measured
behavior and behavior predicted by the system
model. A survey of detection schemes for the class of stochastic,
linear discrete-time systems is presented in [2]. The
first scheme, known as failure-sensitive filters, alters standard
estimation techniques, such as the Kalman filter, so
that state estimates are sensitive to particular failures, for
instance the failure of a sensor or actuator. Another class of
detection methods are voting schemes, which leverage the
potential presence of redundant hardware to detect failures.
The multiple hypothesis-testing scheme performs statistical
tests on received measurements to determine
whether any of several postulated failure modes, for
instance, potential failure directions of the state, have
occurred. The final detection schemes considered are based
on the analysis of innovations, which are the differences
between observed and expected data, for instance the |2
test discussed later in this article. All these schemes are
characterized in terms of complexity, flexibility in the types
of failures modes that can be detected, and performance,
both in detecting the presence of a fault as well as identifying
the source and magnitude of a fault. While the given
approaches can detect random failures, these methods are
less effective against malicious attacks on control systems.
This conclusion stems from the observation that while
random faults can be seen as a special class of attacks, the
set of intelligent attacks is much richer. Therefore, faultdetection
algorithms may fail under the attack of an intelligent
adversary.
For example, traditional bad-data detection techniques,
such as the largest residue test [3], have been widely used
for systems with a static model, such as the power grid.
Here a linear model with Gaussian noise is often used to
directly relate the state (bus angles) to the outputs (power
flows or power injections). However, an attacker who is
aware of the grid's configuration can inject a stealthy input
into the measurements to change state estimation on the
power grid [4]. This type of attack, known as a false-data
injection attack, is implemented by inserting errors that lie
in the range space of the observation matrix into sensor
measurements. Alternatively, an adversary could have
access or resources sufficient to modify only a subset of
sensors. Within this context, an adversary may either want
to insert a bias along a specific direction into the state estimate
or may be indifferent to the direction of the error
inserted into the state estimate. For each scenario, algebraic
conditions are provided to verify the existence of stealthy
attack vectors, which result in no change to the residue.
Moreover, methods to compute such feasible attack vectors
are provided. Furthermore, [4] similarly introduces and
analyzes generalized false-data injection attacks where
small additional errors in the residue are tolerated to
achieve more powerful attacks.
Alternatively, [5] considers false-data injection attacks
on the grid from the system operator's point of view. Two
security indices are proposed to quantify the least effort
needed for an adversary to achieve a feasible attack while
avoiding triggering the bad-data detector. The first index
characterizes the total number of sensors that must be
modified to insert a bias into a single given sensor measurement.
The second index roughly characterizes the
energy required to bias a single sensor measurement. These
indices allow a system operator to identify potentially
sparse attacks and allocate additional resources such as
redundant sensors or encryption schemes as needed.
On the other hand, dynamical system models raise additional
challenges for an adversary. Here, to remain undetected,
an adversary must choose attack vectors that are
consistent not only with the static observation model but
also with state dynamics at all times. In [6], a general continuous-time
control system is considered. The adversary here
can insert arbitrary errors to an unknown subset of sensors
and actuators. Notions of attack detectability and identifiability
are considered. In particular, algebraic conditions are
provided that indicate if the defender can detect and identify
all attacks for a given set of vulnerable sensors and actuators.
Additionally, graphical conditions are provided to characterize
undetectable attacks. Furthermore, centralized and
distributed failure-sensitive filters to perform attack detection,
as well as a centralized filter to perform attack identification,
are proposed. The filters provide perfect detection
and identification, when feasible. The results given are
applied to a dynamic model of the power grid.
The problem of robust control and estimation in the
presence of adversaries is considered in [7]. It is shown that
perfect estimation is infeasible when an adversary can
insert arbitrary errors into at least half of the sensor measurements.
Moreover, changing system dynamics through
state feedback can allow the defender to perform perfect
estimation when less than half of the sensor measurements
are altered. The limiting assumption here is that there exists
a local controller with complete access to the state. Next, in
a scenario where an adversary can arbitrarily manipulate
up to q sensors and actuators, algebraic conditions under
which the defender can perform perfect state estimation
and identify the attack vectors are derived. The main result
of [7] is the establishment of a principle of separation of estimation
and control while under sensor attacks for a plant
with output feedback. Finally, an efficient practical decoder
FEBRUARY 2015 « IEEE CONTROL SYSTEMS MAGAZINE 95
This article proposes a control-theoretic method, called physical
watermarking, to authenticate the correct operation of a control system.
to perform this estimation is given in the case of just sensor
attacks as well as sensor and actuator attacks.
The feasibility and detection of cyberphysical attacks
has also been considered in distributed systems. For example,
[8] considers the setting of a wireless control network,
which consists of a set of nodes with a sparse underlying
communication network. A subset of nodes communicates
directly with sensors, a subset of nodes communicates
directly with actuators, and stabilizing output feedback is
performed using distributed linear iterations. Additionally,
an unknown subset of nodes is malicious and can
inject arbitrary errors into its components of the state. The
design of an intrusion detection system that can recover
sensor outputs for data-logging purposes and can identify
malicious nodes is considered. Furthermore, both algebraic
and graphical conditions are derived to determine the feasibility
of perfect estimation and detection. Moreover, a
systematic procedure to estimate sensor outputs and identify
malicious nodes is proposed. Additionally, [9] and [10]
consider the feasibility and detection of attacks in distributed
systems. Specifically, [9] considers a system where
nodes attempt to compute functions of their initial states
by means of distributed linear iterations, assuming there
exists a subset of malicious nodes that broadcast incorrect
values of their states to their neighbors. It is proven that a
given node calculating an arbitrary function can tolerate
up to f faulty agents if and only if there exists at least
2f + 1 vertex disjoint paths to any nonneighboring node.
Moreover, a combinatorial procedure to determine the
entire initial state is provided. Alternatively, [10] considers
the special case of consensus algorithms where a set of
agents all attempt to compute the same function of their
initial states through distributed linear iterations. That
work considers a scenario where there exist malicious
agents who collude and may know the structure of a network
and a scenario with faulty agents who do not collude.
Here, algebraic and graphical conditions are given for
when faulty and malicious agents can be detected and
identified. Reference [10] also characterizes the effect
unidentifiable inputs have on the consensus value and propose
three failure-sensitive filters to detect and identify
malicious or faulty nodes.
In scenarios [6]-[10], the attacker can either arbitrarily
perturb the system along certain directions without being
detected by any filter or cannot induce any perturbation
without incurring detection. However, in these contributions,
the system model is assumed to be noiseless, which
greatly favors the failure detector, since the evolution of the
96 IEEE CONTROL SYSTEMS MAGAZINE » FEBRUARY 2015
system is deterministic and any deviation from the predetermined
trajectory can be detected. A more realistic scenario
needs to account for a noisy environment. In this
case, it is harder to detect malicious behavior since the
adversary may inject an attack that renders the compromised
system statistically indistinguishable from the
healthy system.
In [11], the authors consider attacks on control systems
in a noisy environment. The adversary in this system is
aware of the plant model, noise statistics, and the controller
and state estimator. The attacker can also manipulate a
subset of sensors. Necessary and sufficient conditions are
derived for the feasibility of a dynamic false-data injection
attack where an attacker can cause unbounded errors in
the state estimate without substantially increasing the
probability of detection by a residue detector. Additionally,
an algorithm to perform such an attack is derived. This
method involves rendering unstable modes in the system
unobservable. Using redundant sensors to measure unstable
modes is suggested as a method to improve resilience
to such an attack. Similarly, [12] considers false-data injection
attacks in a noisy wireless sensor network without
control inputs, where again the adversary has access to the
system model as well as a subset of sensors. Here, a bound
is given for the reachable region of the estimator biases that
an adversary can inject without substantially increasing
probability of detection. The reachable region is characterized
by formulating the attack as a constrained optimal
control problem.
In [13] and [14], the replay attack model inspired by the
Stuxnet example is analyzed. Here, an adversary can read
and modify all sensor signals in the system, which is assumed
to be in steady state. The attacker, rather than causing
physical damage to the system by perturbing sensor measurements
along specific directions, as is done in a false-data
injection attack discussed above, can insert a harmful input
into the system. To evade detection, the attacker replays previous
sensor measurements to the operator. These outputs
are statistically identical to the true outputs at steady state.
Furthermore, unlike the false-data injection attack, the adversary
requires no knowledge of the system model to generate
stealthy outputs. Because the adversary hijacks all sensors,
resilient control such as in [7], cannot stabilize the system,
and, as such, the main focus in countering a replay attack is
detection. In [13] and [14] a physical authentication scheme is
proposed, where a random “watermark” signal is added to
the optimal control signal. A digital watermark, traditionally
seen in audio and image processing, embeds information in
t able 1 a summary of related works in control system security. t he works are
characterized by the system models considered, adversary attack models, and
defense mechanisms to detect and identify adversaries. t his table does not
exhaustively summarize all contributions in control system security, and apologies
are extended for any omissions.
r eference t ype of s ystem
noise attack models
Control system
Noisy Faults
Noisy False-data injection
(sensor attack)
Noisy False-data injection
(sensor attack)
None Arbitrary sensor and
actuator attacks
Detection and
identification filters
None Arbitrary sensor and
actuator attacks
Optimization decoder
a carrier signal that is later used to
verify authenticity or integrity of
the owner. One application of digital
watermarking is in source
tracking of illegally copied movies,
where a watermark is used to determine
the owner of the original
signal. Similarly, in [13] and [14], if
the system is operating normally, [2]
then the effect of the chosen watermark
signal is present in the sensor
measurements. However, if the [4]
system is malfunctioning or under
attack, the effect of the watermark [5]
signal chosen by the system operators
cannot be detected. Conceptu- [6]
ally, this approach is similar to a
challenge-response authentication [7]
scheme in information security,
where the watermark signal and [8]
the sensor measurements can be
seen as the “challenge” and “response,”
respectively. Table 1 sum- [9]
marizes previous work mentioned
in this article related to control [10]
system security. Note, however,
that Table 1 does not exhaustive- [11]
ly  summarize all previous work
in control system security, and
apologies are extended for any [12]
omissions.
This article further investigates
the problem of designing [13], [14]
the optimal watermark signal in
the class of stationary Gaussian
processes to maximize a relaxed
version of the expected Kullback-Leibler (KL) divergence
between the distributions of the compromised and healthy
residue vector, while satisfying a constraint on the control
performance. This approach can be seen as a generalization
of [13] and [14], where only independent and identically distributed
(iid) Gaussian processes are considered.
Observe that the fundamental requirements of physical
security in CPSs are applicable to the notion of security in
general control systems. As such, the watermarking scheme
described in this article is applied to the class of discrete
linear time-invariant (LTI) state-space models. The optimization
problem, when carried out in the frequency domain,
can be separated into two steps, where the optimal direction
of the signal for each frequency is first computed, and
then all possible frequencies are considered to find the
optimal watermark signal.
The rest of the article is organized as follows. First, the
system description is given. Here the linear-quadraticGaussian
(LQG) controller is revisited and adapted, the conStatic
power grid
Static power grid
Control system
Control system
Consensus
network
Control system
Sensor network
Defense mechanisms
Filters, voting schemes,
hypothesis testing,
residue detectors
Residue detector
Residue detector
Combinatorial estimator
Detection and
identification filters
Residue detector
Residue detector
Physical watermarking,
\2, correlation detectors
Wireless control
network
None Malicious nodes with Intrusion detector,
arbitrary state attacks output estimation
Distributed network None Malicious nodes with
arbitrary state attacks
None Malicious or faulty
nodes
Noisy Dynamic false-data
injection (sensor
attack)
Noisy Dynamic false-data
injection (sensor
attack)
Control system
Noisy Replay attack
cept of a watermarked input is described, and properties of
the failure detector are defined. In the next section, a replay
attack model is provided, and the class of systems incapable
of detecting such attacks are identified. In the following section,
a systematic method to compute the watermarking
signal is given. Specifically, a Neyman-Pearson detector is
derived, and optimal statistical properties of the watermark
input are obtained subject to some upper bound on the total
cost of control. Due to the impossibility of computing the
detection probability in closed form, only a relaxed version
of the original optimization problem is solved. While the
solution provided is near optimal with respect to the originally
formulated optimization problem, the rest of the article
still refers to the solution as the optimal watermark for
easier reading. An algorithm to generate this input is also
given. Afterward, a numerical example is provided to compare
probability of detection with the probability of false
alarm as well as the cost of control. The final section concludes
the article, with some directions for future work.
FEBRUARY 2015 « IEEE CONTROL SYSTEMS MAGAZINE 97
S = AT SA + W - AT SB^BT SB + Uh-1 BT SA.
(9)
Consider the case where, instead of directly applying
the optimal LQG control u*k to the physical system, a physical
watermarking scheme is used, in which the true control
input uk is given by
uk = u*k + gk,
(4)
(5)
(6)
(7)
(8)
Remark 1
It is worth noticing that {gk} is completely described by its
finite-dimensional distribution and hence the autocovariance
function C. However, the watermarking signal is
restricted to be generated from an HMM since any autocovariance
function C can be approximated by an HMM,
given that the dimension nh of the hidden state is large
enough. On the other hand, the HMM is easy to implement
if nh is small, which is the case for the optimal watermarking
signal, as is illustrated later by Theorem 6.
To ensure the freshness of the watermark signal, Ah is
assumed to be strictly stable, which implies that the correlation
between the current watermark signal gk and the
future watermark signal gk' decays to zero exponentially
when k' - k " 3. The spectral radius of Ah is denoted as
t^Ahh < 1. In this article, it is assumed that the watermark
signal is chosen from an HMM with t^Ahh # t, where
t < 1 is a design parameter. A value of t close to one gives
the system operator more freedom to design the watermark
signal, while a value of t close to zero improves the freshness
of the watermark signal by reducing the correlation of
gk at different time steps. To simplify notation, define the
feasible set G(t) as
where }k ! Rnh, k ! Z is a sequence of iid zero-mean GaussJ
= lTi"m3 E 2T1+ 1 =k =/T-T ^xkT Wxk + ukT UukhG, (3) itahne rhaindddoemn svtaatreia.bTloesmwaikthe c{ogvk}araiasntcaetioWn,arayndprpokc!esRs,nhthies
covariance of p0 is assumed to be the solution of the followwhere
W, U are positive definite matrices. Since the separa- ing Lyapunov equation
tion principle holds in this case, the optimal solution of (3)
is a combination of the Kalman filter and LQG controller Cov^p0h = Ah Cov^p0hAhT + W.
[15]. The Kalman filter provides the optimal state estimate
xtk . Since the system is assumed to start at -3, the Kalman All the matrices are of proper dimensions.
filter converges to a fixed-gain linear estimator
xtk+1|k = Axtk + Buk,
xtk = xtk|k-1 + Kzk,
u*k = Lxtk,
L _ -^BT SB + Uh-1 BT SA,
where u)k is the optimal control input. The control gain
matrix L is defined to be
SYSTEM dESCRI p TION
The physical watermarking strategy is given for a class of
general control systems. The control system is modeled as
LTI, the state dynamics of which are given by
xk+1 = Axk + Buk + wk,
(1)
where xk ! Rn is the vector of state variables at time k,
uk ! Rp is the control input, and wk ! Rn is the process
noise at time k. wk is assumed to be an iid Gaussian process
with wk + N^0, Qh. Since the control system usually operates
for an extended period of time, it is assumed that the
system starts at time -3.
A sensor suite monitors the system described in (1). At
each step, all the sensor readings are collected by a base
station. The observation equation can be written as
yk = Cxk + vk,
(2)
where yk ! Rm is a vector of measurements from the sensors
and vk + N (0, R) is iid measurement noise independent
of wk . It is assumed that ^A, Bh is stabilizable and
^A, Ch is detectable.
It is further assumed that the system operator wants to
minimize the infinite-horizon LQG cost
where S satisfies the Riccati equation
98 IEEE CONTROL SYSTEMS MAGAZINE » FEBRUARY 2015
where zk _ yk - Cxtk|k-1 is the residue vector and the
Kalman gain K is given by
K _ PCT ^CPCT + Rh-1,
where P is the solution of the Riccati equation
P = APAT + Q - APCT ^CPCT + Rh-1 CPAT .
The estimation error at time k is defined to be ek = xk - xtk .
The LQG controller is a fixed-gain linear controller
based on the optimal state estimate xtk . Specifically,
where u*k is the optimal LQG control and gk is the watermark
signal. The watermark signal {gk} is assumed to be a
p-dimensional stationary zero-mean Gaussian process
independent from the noise processes {wk}, {vk} . Define the
autocovariance function C: Z " Rp# p to be
In this article, the watermark is assumed to be generated
by a hidden Markov model (HMM)
C (d) _ Cov^g0, gdh = Eg0 gdT .
pk+1 = Ah pk + }k, gk = Ch pk,
(10)
(11)
(12)
G^th ={C: C is generated by an HMM (12) with t^Ahh # t} .
(13)
Remark 2
Since it is assumed that ^A, Bh is stabilizable and ^A, Ch is
detectable, the closed-loop system is stable regardless of the
watermark signal. Furthermore, by the separation principle,
the Kalman filter is the optimal filter regardless of the watermark
signal gk . However, the addition of gk incurs an LQG
control performance loss and the control input uk is not optimal.
The necessity of adding the watermark signal gk is
illustrated later in Theorem 1. Conceptually, if the system is
under normal operation, then the effect of the watermark
signal gk can be found in the sensor measurements yk . The
presence of the watermark is possibly lost when the system
is malfunctioning or under attack, which can be detected by
the failure detector.
If no watermark signal is present, that is if gk = 0, then
the optimal objective function J* given by the Kalman filter
and LQG controller is
J* = tr^SQh + tr6^AT SA + W - Sh^P - KCPh@.
(14)
A failure detector is used to detect abnormality of the
system. In this article, the failure detector is assumed to
trigger an alarm at time k if and only if the condition
g (zk, gk-1, gk-2, f) $ h,
is met where g (zk, gk-1, gk-2, f) is a continuous real-valued
function of zk, gk-1, gk-2 , f and h is the threshold, which is
a design parameter of the system. Under normal operation,
denote the probability of false alarm to be a, defined as
a _ P (g (zk, gk-1, gk-2 , f) $ h) .
False alarms usually occur with low probability for practical
systems. When the system is operating normally, zk is a
stationary process and hence a is a constant.
Remark 3
A widely used failure detector is the |2 detector [16], [17],
which satisfies
g (zk, gk-1, gk-2, f) = zkT (CPCT + R) -1 zk .
Figure 1 shows the system diagram described in this section.
ATTACk M Od EL
In this section, a model for a replay attack motivated by
Stuxnet is given. To cause physical damage, a first version
of Stuxnet implements control logic to increase pressure in
the centrifuge while a second version of the worm varies
rotor speeds. To prevent detection in the first scenario,
(15)
(16)
Stuxnet replayed previous sensor outputs, recorded prior
to inserting harmful control inputs, to the SCADA system
[1]. Since the system was in steady state, outputs from the
past, collected in steady state, were statistically identical to
outputs under normal operation, and as such were not
detected. Motivated by Stuxnet, the following replay attack
model is considered in this article.
Attacker's Knowledge and Resources
The adversary is first described in terms of its knowledge
and available resources.
1) The attacker has knowledge of all sensor measurements
in real time. In particular, the attacker knows
the true sensor outputs yk for all k.
2) The attacker can violate the integrity of all sensor measurements.
Specifically, the attacker can modify the
v
true sensor signals yk to arbitrary sensor signals yk .
Remark 4
The attack on the sensors can be carried out by breaking
the cryptography algorithm. Another way to perform
an attack, which is potentially much harder to
defend, is to use physical attacks. Physical attacks can
violate the basic properties of secrecy, integrity, and
availability without the need to attack the cyber part
of the system. Consider, for example, a temperature
sensor. The secrecy, integrity, and availability of its
sensing data can be affected by placing a sensor
nearby, affecting the local temperature around the
sensor, and enclosing the sensor with a metal cover,
respectively. In addition, the insider threat is critical
in large infrastructures, as these systems usually
involve many employees. These kinds of attacks may
be easy to carry out when sensors are spatially distributed
in remote locations.
3) The attacker has access to a set of external actuators
with control matrix Ba ! Rn# pa and can thus insert an
uk
gk
Plant
z-1
Sensor
uk-1
yk
u*k
^
LQG Controller xk Estimator
Authentication Signal g
k
Failure Detector
zk
f igure 1 A diagram of the control system under normal operation.
In this system, no adversary is present, and, as a result, the watermark
input is present in the sensor outputs. By confirming the
presence of a watermark in the sensor measurements, the failure
detector can verify that the system is not under a replay attack.
FEBRUARY 2015 « IEEE CONTROL SYSTEMS MAGAZINE 99
external input Ba uka where uka ! Rpa is the control
input. Moreover, assuming that uka is intelligently
chosen, the set of actuators Ba allows the adversary
to achieve a malicious objective, for instance, causing
physical damage to the plant.
Remark 5
The attacker could inject the external control input by
controlling a subset of actuators of the system and/or
deploying its own actuators. For example, to change
the temperature distribution in a building, the
attacker could take control of the heating, ventilation,
and air conditioning system, deploy heaters of its
own, or even commit arson.
4) The attacker does not need to have full knowledge of
the system parameters, namely the A, B, C, Q, R, K, L
matrices and the C function. However, the attacker
has enough knowledge of the system model to design
an input uka ! Rpa that may achieve its malicious
objective, such as physically damaging the plant.
Attack Strategy
Given the adversary's knowledge and resources, the following
attack strategy is considered.
1) The attacker records a sequence of sensor measurements
from time -T to time -1, where T is a large
enough number to ensure that the attacker can replay
the sequence later for an extended period of time.
2) Starting at time 0 to time T - 1, the attacker modifies
v
the sensor signals to yk, which is the same as the
measurements recorded by the attacker at time k - T.
In other words,
ykv = yk-T, 0 # k # T - 1.
uk
gk
Authentication
Signal
gk
uak
Plant
z-1
LQG
Controller
Failure
Detector
Sensor
uk-1
x^k Estimator
zk
Time-Shifted
System
f igure 2 A diagram of the control system under attack. Here, the
v
adversary performs a replay attack, providing replayed outputs yk
to the system operator while injecting a potentially damaging input
to the system. When under attack, the watermark is asymptotically
independent of the replayed sensor measurements. The failure
detector leverages this independence to determine if there has
been a replay attack.
100 IEEE CONTROL SYSTEMS MAGAZINE » FEBRUARY 2015
Remark 6
For simplicity, the time that the replay starts is
denoted as time 0. In reality, the attacker can freely
choose the starting time, which is unknown to the
system operator.
3) Starting at time 0, the attacker injects an external control
input Ba uka, where uka ! Rpa is the control input
and Ba ! Rn# pa denotes its direction.
Remark 7
When the system is under attack, the controller
cannot perform closed-loop control since the true
sensory information is not available. Therefore, control
performance of the system cannot be guaranteed
during the attack. In fact, the attacker can inject a bias
on the state of the physical system along its controllable
subspace, which is the column space of
[Ba, ABa, f, An-1 Ba] . The only way to counter this
attack is to detect its presence.
System Model Under Attack
To simplify notation, time-shifted variables
xtk|k-1 _ xtk-T|k-T-1, zkv = zk-T, gvk = gk-T, 0 # k # T - 1, (17)
v
are defined. During the replay ^0 # k # T - 1h, the system
dynamics change to
xk+1 = Axk + Buk + Ba uak + wk, yk = Cxk + vk,
xtk+1|k = Axtk + Buk, xtk = xtk|k-1 + K^ykv - Cxtk|k-1h,
uk = Lxtk + gk, zk = ykv - Cxtk|k-1 .
(18)
(19)
(20)
Notice that the fake measurement ykv is used instead of yk
for calculating the state estimate and residue. In addition,
the probability of detection at time k is defined to be bk
given as
bk _ P^g^zk, gk-1, gk-2 , fh $ hh,
0 # k # T - 1.
(21)
Figure 2 shows the diagram of the control system under
attack.
The following theorem characterizes the feasibility of
the replay attack in the absence of the watermark signal gk,
which illustrates the necessity of the physical watermark.
Theorem 1
Suppose gk = 0 for all k. If A _ ^A + BLh ^I - KCh is stable
and t^^A + BLh^I - KChh 1 1, then the detection rate bk of
all detectors g converges to the false alarm rate a during
the attack, that is,
On the other hand, if A is strictly unstable and g satisfies
lki"m3 bk = a.
lzi m"3 g^z, 0, 0, fh = 3
(22)
(23)
lki"m3 bk = 1.
Proof
Part of the proof is reported in [13]. However, for the sake of
completeness, the whole proof is included here. Manipulating
(17)-(20) yields
for some norm $ , then the detection rate bk converges to
one, that is,
xtk+1|k = Axtk|k-1 + (A + BL) Kykv + Bgk,
xtk+1|k = Axtkv|k-1 + (A + BL) Kykv + Bgkv,
v
zk+1 = zkv+1 -CAk+1 ^xt0|-1 - xt0|-1h
v
k
- C / Ak-i B^gi - givh.
i =0
If A is stable and gk = gkv = 0, then the residue zk of the
system under the replay attack converges to the residue zkv
of the virtual system, which is essentially zk-T . Hence,
lki"m3 bk = lki"m3 P (g (zk, 0, 0, f) $ h) = P (g (zkv, 0, 0, f) $ h)
= P (g (zk-T, 0, 0, f) $ h) = a.
On the other hand, if A is strictly unstable, the second
term on the right-hand side (RHS) of (27) goes to infinity
almost surely. Hence, if g (z, 0, 0 , f) " 3 when z " 3,
lki"m3 bk = lim P (g (zk, 0, 0 , f) $ h) = 1,
k " 3
which concludes the proof.
Remark 8
Notice that the stability of the “healthy” system depends
only on the A + BL and A - KCA matrices, not on A.
Hence, it is entirely possible that the closed-loop system is
stable while A is unstable. As seen from (25) and (26), the
stability of A implies that the open-loop cybersystem, consisting
of the controller and estimator, is stable. In the one
dimensional case, the stability of A is easy to analyze since
A = (A + BL) (A - KCA) A-1 . Thus, due to the stability of
A + BL and A - KCA, A is stable if A is unstable. Such
analysis does not hold for higher dimensional systems
since the product of stable matrices may not be stable.
Remark 9
Additionally, observe that Theorem 1 considers the alarm
rate bk when k goes to infinity while in the attack model it
is assumed that the replay is performed from time 0 to time
T - 1. However, since T is assumed to be large and bk typically
converges quickly, as is illustrated by the numerical
examples, the asymptotic performance of bk serves as an
indicator of the detection performance of the system.
Based on Theorem 1, if A is strictly unstable, then the
attacker can be detected efficiently as the detection rate bk
converges to one. However, if A is stable, then the attacker
(24)
(25)
(26)
(27)
4
can perform the replay attack for an extended period of
time given that the false alarm rate a is insignificant, which
implies that the system is not resilient to this type of attack.
In that case, one possible countermeasure is to redesign the
estimation and control gain matrices K and L so that the
closed-loop system is stable, while enforcing A strictly
unstable. However, this approach is not always desirable,
since the control and estimation gain matrices are usually
designed to satisfy certain safety and performance constraints
and hence cannot be changed arbitrarily. In these
scenarios, instead of redesigning K and L, the watermark
signal can be used to enable intrusion detection.
WATERMARk dESIGN ANd dETECTION
This section is devoted to developing a design methodology
for the watermark signal and the anomaly detector.
To begin, the following assumption is made on the control system.
Assumption 1
A is stable. That is, t ((A + BL) (I - KC)) < 1.
Throughout this section, it is assumed that A is stable,
since otherwise the watermark signal would be unnecessary
as a consequence of Theorem 1. To simplify notation,
define the symmetric part of a matrix X as
sym (X) _ X + XT .
2
LQG Performance Loss
The addition of noisy watermarks on top of optimal LQG
inputs naturally degrades the performance of the system as
described by the LQG cost. The following theorem provides
the LQG control performance loss incurred by the
watermark signal.
Theorem 2
The LQG performance of the system described by (1), (2),
(4), and (10) is given by
J = J* + DJ,
where J* is the optimal LQG cost without the watermark
signal and
3
DJ = tr'UC^0h + 2U sym ;L / ^A + BLhd BC^1 + dhE1
d =0
+ tr6^W + LT ULhH1@,
where
3
H1 _ 2 / sym6(A + BL) d L1 (C (d))@ - L1 (C (0)),
d =0
and L1: Cp# p " Cn# n is a linear operator defined as
3
L1 (X) = / (A + BL) i BXBT ((A + BL) i) T
i =0
= (A + BL) L1 (X) (A + BL) T + BXBT .
(28)
(29)
(30)
FEBRUARY 2015 « IEEE CONTROL SYSTEMS MAGAZINE 101
pROOf Of ThEOREM 2
he healthy control system follows
;xk+1E = ;A +BL
ek+1 0
A -KCAE;exkkE +;I -KC -KE;vwk+k1E +; 0 E,
-BL I 0 Bgk
uk = Ltxk +gk = Lxk -Lek +gk .
Since the control system is closed-loop stable, {xk}, {ek}, and
{uk} are all stationary Gaussian processes. Hence,
J = E(x1T Wx1 +u1T Uu1) = tr(W Cov(x1)) +tr(U Cov(u1)).
3
x1 = l1(w0,w-1,f,v0,v-1,f) + / (A +BL)i Bg-i,
i=0
e1 = l2(w0,w-1,f,v1,v0f),
where l1 and l2 are linear functions. As a result,
3
u1 = l3(w0,w-1,f,v1,v0,f) +L / (A +BL)i Bg-i +g1,
i=0
where l3 is another linear function. Since the watermark signal is
independent from the process noise {wk} and sensor noise{vk},
(S1)
(S2)
Proofs
T
and
By (S1),
and
where
Cov(x1) = Cov^l1(w0,w-1,f,v0,v-1,f)h
3
+Covc / (A +BL)i Bg-im
i=0
Cov(u1) = Cov^l3(w0,w-1,f,v1,v0,f)h
3
+CovcL / (A +BL)i Bg-i +g1m.
i=0
pROOf Of ThEOREM 4
By the definition of KL divergence, it is known that
DKL(N1 N0) = 12 tr6(P +R)P-1@ - m2
- 12 log det6(P +R)P-1@ + 21 nkT P-1nk
= 12 tr(RP-1) - 12 log det(I +RP-1) + 12 tr(nk nkT P-1).
Take the expectation on both sides. This implies R = E nk nkT,
which proves (37). Now assume that the eigenvalues of RP-1
are m1,f,mm. As a result,
m
tr(RP-1) = / mi
i=1
and
m
log det(I +RP-1) = / log(1 +mi).
i=1
Since P is positive semidefinite, there exists a positive semidefinite
matrix P1/2, where P1/2P1/2 = P. Hence, RP-1 shares the
same eigenvalues as P-1/2RP-1/2, which implies all mi s are real
and nonnegative. As a result, by the concavity of the log function,
it is known that
log61 +tr(RP-1)@ # log det(I +RP-1)
# m logc1 + tr(RmP-1) m # tr(RP-1). (S4)
The first inequality holds when m1 = tr(RP-1) and m2 = g =
mm = 0. The second inequality holds when m1 = g = mm = tr
(RP-1)/m The third inequality uses the fact that log(1 +x) # x.
Combining (S4) and (37), (38) holds.
Furthermore, if C is of rank one, then by (35),
rank(RP-1) # rank(R) # 1.
By the fact that when gk = 0, the optimal LQG cost is J) and
J = J) +DJ,
As a result, the first inequality of (S4) is tight, which implies that
the upper bound in (38) is tight. 4
3
DJ = tr;W Covc / A +BL)i Bg-imE
i=0
3
+tr;U CovcL / (A +BL)i Bg-i +g1mE. (S3)
i=0
Manipulating the RHS of (S3) leads to (30), which finishes the
proof. 4
pROOf Of ThEOREM 6
The proof is divided into steps.
Step 1
+
Consider another function C and prove that this function is indeed
an autocovariance function. Define function Cu:Z " Rp#p as
See “Proofs” for theorem proofs that would otherwise
interrupt the flow of the article.
Remark 10
While the expression for DJ is complicated, it is linear with
respect to the autocovariance functions C (d). This linearity
enables optimization in the frequency domain, as is illustrated
in Theorem 6.
102 IEEE CONTROL SYSTEMS MAGAZINE » FEBRUARY 2015
Optimal Detector
This subsection derives the asymptotically optimal detector. As
seen from Figure 1, the detector has real-time knowledge of the
residue zk, obtained from the estimator, as well as real-time
knowledge of the trajectory of the watermark, {gk} . Define the
covariance of the residue zk of the healthy system to be
P _ CPCT + R.
(31)
Cu (d) _ t-|d|C(d).
(S5)
It can be shown that C+ is the autocovariance function of the
stationary Gaussian distribution,
where Cov(pu0) is the solution of the Lyapunov equation,
puk+1 = ^Ah/thpuk +}uk, guk = Chpuk,
Cov(pu0) = AhCov(pu0)AhT +W,
and {}uk} is an iid zero-mean Gaussian process with covariance
equal to Cov(pu0) -AhCov(pu0)AhT /t2.
Step 2
Rewrite the objective function and the constraint (40) in terms
of the Fourier transform ou of C+. Consider a partition of [0, 1/2]
into disjoint intervals I1,I2,g,Iq, where
q
Ii ( I j = 4, ' Ii = [0, 12 ].
i=1
Define v as the maximum length of interval Ii s. By RiemannStieltjes
integral and Theorem 5, C+(d) can be written as
+ q
C(d) = lvim"0 20e=/ exp(2rjd~i)ou(Ii)G,
i=1
where ~i ! Ii . By (35) and (S5),
q 3
R = lvim"0 Ci/=1 2 0e{2 / sym [exp^2rjd~ih^tAhd
d =0
# L2 ^ou^Iihh] -L2 ^ou^Iihh}CT,
q
= lvim"0 Ci/=1 2 0e{2 sym[^I -exp^2rj~ihtAh-1
# L2 ^ou^Iihh] -L2 ^ou^Iihh}CT,
q
= lvim"0 Ci/=1 F2 ^~i,ou^IihhCT .
Notice that the order of summation and limit changes, which is
feasible because A is stable. As a result,
Similarly,
q
tr(RP-1) = lim / tr6F2(~i,ou(Ii))CT P-1C@.
v"0 i=1
q
DJ = lvim"0 i/=1 F1(~i,ou(Ii)).
For the healthy system, zk is Gaussian distributed with
mean zero and covariance P.
By (27), for the system under the replay attack,
v k
zk+1 =-CAk+1 ^xt0|-1 - xt0|-1h - C / Ak-i Bgi
i =0
k
+ C / Ak-i Bgiv + zkv+1 .
i =0
(S6)
(S7)
(32)
Step 3
Prove that the upper bound for (40) is the optimal value of the
objective function of (45). Since DJ and R are always nonnegative,
for all ~ ! [0,1/2] and H positive semidefinite,
F1(~,H) $ 0, F2(~,H) $ 0.
(S8)
Suppose that the optimal solution of (45) is ~), H) and the optimal
value of the objective function is {. Since F1 and F2 are
linear with respect to H,
F1(~),H)) = d.
Hence, for all ou(Ii) and ~i ! [0,1/2],
tr6F2(~i,ou(Ii))CT P-1C@ # {d F1(~i,ou(Ii)).
(S9)
By (S6)-(S9), for all watermark signals {gk} with DJ # d,
tr(RP-1) # {.
Step 4
Prove that the upper bound is tight. Consider the point-mass
measure ou),
ou)(SB) = H)I{~) !SB} + H)I{-~)!SB},
where I is the indicator function. It can be shown that C)(d)
is generated by ou) Furthermore, by (S6) and (S7), the corresponding
DJ = d and tr(RP-1) = {. Hence, C)(d) achieves the
upper bound of (40). The only remaining thing to prove is that
C)(d) can be generated by an HMM with t(Ah) # t. Notice that
the boundary of the cone of positive semidefinite Hermitian matrices
is of the form hhH. Furthermore, since F1 and F2 are
linear with respect to H, for fixed ~, the optimization problem
(45) attains its maximum on the boundary of the cone (though
it is possible that an interior point is also optimal), which proves
(48). As a result,
H) = (hr +jhi)(hTr -jhiT) = hr hTr +hi hiT -j(hr hiT -hi hTr).
It can be shown that the watermark signal {gk} generated by
the HMM (49) follows (44), which proves that (44) is the optimal
autocovariance function for (40).
4
The first term on the RHS of (32) converges to zero since A
is stable. The second term is a function of the watermark
signal, which is generated and thereby known by the control
system and the failure detector. The third and fourth
terms are independent from each other since zk is the residue
vector of the Kalman filter. Further define
k
nk _ -C / Ak-i Bgi, (33)
i =-3
FEBRUARY 2015 « IEEE CONTROL SYSTEMS MAGAZINE 103
To cause physical damage, a first version of Stuxnet implements
control logic to increase pressure in the centrifuge while a second version
of the worm varies rotor speeds.
and
k 3
R _ lim Cov =C / Ak-i BgivG = Cov ;C / Ai Bg-iE.
k " 3 i =0 i =0
Expanding the RHS of (34),
3
R = 2 / C sym6Ad L2 (C (d))@CT - CL2 (C (0)) CT,
d =0
where L2: Cp# p " Cn# n is a linear operator on the space of
p # p matrices, which is defined as
3
L2 (X) _ / Ai BXBT (Ai) T = AL2 (X) AT + BXBT .
i =0
Therefore, zk converges to a Gaussian with mean nk-1 and
covariance P + R. As a result, the null hypothesis is
(34)
(35)
The alternative hypothesis is
H0 : the residue zk follows
a Gaussian distribution N0 (0, P) .
H1 : the residue zk follows a Gaussian
distribution N1 (nk-1, P + R) .
By the Neyman-Pearson lemma [18], the optimal detector
is given by the Neyman-Pearson detector as discussed in
Theorem 3.
The Kullback-Liebler Divergence
Tthe difference between two distributions P1(z) and P0(z)
he KL divergence, first described in [S6], is a measure of
For continuous probability density functions P1 and P0, the KL
divergence is
D(P1 P0) = #z P1(z) log c PP10((zz)) mdz.
(S10)
It can be shown that D(P1 P0) $ 0. Moreover, equality holds
if and only if P1(z) = P0(z) for almost surely all z. Thus, if the
distribution P1(z) is close to P0(z), the KL divergence likely approaches
zero.
The KL divergence between distributions P1 and P0 can be
related to the Neyman-Pearson detector associated with a binary
hypothesis test. Here, consider P1(z) to be the distribution of the
observations z under the alternative hypothesis H1 and P0(z) to
be the distribution of the observations z under the null hypothesis
H0. The optimal Neyman-Pearson detector is a threshold detector
104 IEEE CONTROL SYSTEMS MAGAZINE » FEBRUARY 2015
Theorem 3
The optimal Neyman-Pearson detector rejects H0 in favor
of H1 if
gNP ^zk, gk-1, gk-2, fh = zkT P-1 zk
-^zk - nk-1hT ^P + Rh-1 ^zk - nk-1h $ h.
Otherwise, hypothesis H0 is accepted.
To characterize the performance of the detector, ideally
the asymptotic detection rate limk " 3 bk or expected time to
detection is considered. However, the detection rate and
expected time to detection involve integrating a Gaussian
distribution, which usually does not have an analytical
solution. In this article, the KL divergence, which measures
the “distance” between the two distributions, is used to
characterize the detection performance. This choice rests
on the observation that as the KL divergence between
two distributions increases, the distributions become,
roughly speaking, easier to distinguish. For details, see
“The Kullback-Liebler Divergence.” The KL divergence of
the two Gaussian distributions in H0 and H1 is given by
the next theorem.
Theorem 4
The expected KL divergence of distribution N1 and N0 is
(36)
on the log likelihood l(z) = log^P1(z)/P0(z)h, where if l(z) is greater
than a constant c the alternative hypothesis is chosen. Observe
that the KL divergence D^P1 P0h satisfies
D^P1 P0h = E[l(z) | H1].
(S11)
Thus, maximizing the KL divergence over a subset of possible
distributions P1 potentially increases the probability of an
observation z such that l(z) > c, when the alternative hypothesis
is true. As a result, the probability of detection also increases.
For additional discussion of the relationship between the KL
divergence and Neyman-Pearson lemma, see [S7].
REfERENCES
[S6] S. Kullback and R. Leiber, “On information and sufficiency,” Ann.
Math. Stat., vol. 22, no. 1, pp. 79-86, 1951.
[S7] S. Eguchi and J. Copas, “Interpreting Kullback-Leibler divergence
with the Neyman-Pearson lemma,” J. Multivariate Anal., vol. 97, no. 9,
pp. 2034-2040, 2006.
The watermarking scheme described in this article is applied
to the class of discrete linear time-invariant state-space models.
concave function of C (d) . Hence, the following optimization
problem is solved instead
E DKL ^N1 N0h = tr ^RP-1h - 1 log det^I + RP-1h.
2
(37)
Furthermore, the expected KL divergence satisfies the
inequality
1 tr (RP-1) # E DKL ^N1 N0h # tr (RP-1)
2
- 1 log61 + tr (RP-1)@,
2
where the upper bound is tight if C is of rank one.
It is worth noticing that the expected KL divergence is a
convex function of R. However, both the upper and lower
bound of the expected KL divergence are monotonically
increasing with respect to tr (RP-1), which is linear in R.
Optimal Watermark Signal
This subsection derives the optimal watermark signal. Ideally,
the following optimization problem should be solved.
maximize E DKL ^N1 N0h
C(d) ! G(t)
subject to DJ # d,
where d > 0 is a design parameter.
However, it is computationally hard to solve this maximization
problem since the expected KL divergence is not a
(38)
(39)
1
0.8
k
itcb 0.6
o
t
p
ym 0.4
s
A
0.2
0
0
Stationary Watermark t = 0.6
IID Watermark
0.2
0.4
0.6
0.8
1
a
f igure 3 limk " 3 bk as a function of a for a stationary watermark
with t = 0.6, and an independent and identically distributed (iid)
watermark with DJ = 10. This figure shows that the use of a stationary
Gaussian watermark with t = 0.6 visibly increases asymptotic
detection performance as the rate of false alarm varies,
providing improvements over the iid approach where t is equivalently
zero. With respect to the optimization problem (40), increasing
t corresponds to increasing the set of feasible autocovariance
functions for the stationary watermark.
0.4
0.3
k
b
c
it
tpo 0.2
m
y
s
A
0.1
0
0
100%
k
b
c
it
o
t
p
m
y
s
A
e
h
t
f
o
tn 50%
e
m
e
v
o
r
p
m
I
Stationary Watermark t = 0.6
IID Watermark
0.02
0.04
0.06
0.08
0.1
a
f igure 4 limk " 3 bk as a function of a for a # 0.1 and DJ = 10. It
is desirable to implement detectors with infrequent false alarms in
real-life systems, and consequently detection performance in this
region of operation is essential. The stationary watermarking
scheme with t = 0.6 obtains its best relative performance in comparison
to independent and identically distributed watermarking
schemes when the probability of false alarm approaches zero.
Stationary Watermark t = 0.6
0
0.02
0.04
0.06
0.08
0.1
a
f igure 5 Percentage improvement in limk " 3 bk over the independent
and identically distributed design versus a for a stationary
watermarking scheme with t = 0.6 and DJ = 10. This figure
shows that the stationary watermarking scheme with t = 0.6 can
offer up to a 70% improvement in asymptotic detection performance
when the probability of false alarm is near zero.
FEBRUARY 2015 « IEEE CONTROL SYSTEMS MAGAZINE 105
0.6
k
b
itc 0.4
o
t
p
m
y
s
A 0.2
0
0
20
Stationary Watermark t = 0.6
IID Watermark
40
60
9J
80
100
f igure 6 limk " 3 bk versus DJ for a stationary watermark with
t = 0.6 and an iid watermark. Here, a = 0.02. This figure shows
that as more control effort is expended, the rate of detection
increases. In particular, additional linear quadratic Guassian cost
corresponds to increasing the magnitude of the watermark's autocovariances.
Through the dynamics of the system, watermarks
with larger autocovariances increase discrepancies between the
replayed sensor outputs and the expected sensor outputs, thus
resulting in a higher probability of detection.
Notice that the expected KL divergence is relaxed to
tr (RP-1), using the upper and lower bound derived in Theorem
4. Furthermore, if C is of rank one, then by Theorem
4, optimizing tr (RP-1) is equivalent to optimizing the
expected KL divergence. For general cases, the optimality
gap can be quantified using the upper and lower bound.
Although R and DJ are linear functionals of C, convex
optimization techniques cannot be directly applied to solve
(40), since C is in an infinite dimensional space. As a result,
(40) is transformed into the frequency domain. Before continuing
on, the following definition is needed.
Definition 1
o is a positive Hermitian measure of size p # p on the interval
(-0.5, 0.5] if for a Borel set SB 3 (- 0.5, 0.5], o (SB) is a
positive semidefinite Hermitian matrix with size p # p.
The following theorem establishes the existence of a frequency
domain representation for C (d) .
Theorem 5 (Bochner's Theorem [19], [20])
C (d) is the autocovariance function of a stationary Gaussian
process {gk} if and only if there exists a unique positive
Hermitian measure o of size p # p, such that
C (d) = #
1/2
-1/2
exp (2rjd~) do (~) .
(41)
106 IEEE CONTROL SYSTEMS MAGAZINE » FEBRUARY 2015
maximize tr (RP-1)
C(d) ! G(t)
subject to DJ # d.
(40)
do (~) can be interpreted as the discrete-time Fourier
transform of the function C (d) . In fact, if o (~) is absolutely
continuous with respect to the Lebesgue measure, then
d o (~) = f (~) d ~, and
0.4
k 0.3
b
c
it
o
tp 0.2
m
y
s
A
0.1
Stationary Watermark t = 0.6
IID Watermark
0
0
5
10
9J
15
20
f igure 7 limk " 3 bk versus DJ for a stationary watermarks with
t = 0.6 and an independent and identically distributed watermark.
Here, a = 0.02 and DJ # 20, which is roughly 86% of the optimal
linear quadratic Guassian (LQG) cost. It is desirable to implement
a watermark with limited additional LQG cost in real systems to
maintain performance in the control system. This figure illustrates
the tradeoff between control and detection performance within
this desired region of operation.
C (d) = #
1/2
-1/2
exp (2rjd~) f (~) d~,
where f is a mapping from (-0.5, 0.5] to the set of positive
semidefinite Hermitian matrices. f is exactly the “entrywise”
Fourier transform of C (d) .
By the fact that C (d) is real, the Hermitian measure o
satisfies the following property, which can be applied to
the Fourier transform of the real-valued signals.
Proposition 1
C (d) is real if and only if, for all Borel-measureable sets,
SB 3 (- 0.5, 0.5],
o (SB) = o (- SB) .
By (42), (41) can be simplified as
C (d) = 20ec #
0
1/2
exp (2rjd~) do (~) m.
Theorem 6
The optimal solution (not necessarily unique) of (40) is
C) (d) = 2t d 0e6exp (2rjd~)) H)@,
where ~) and H) are the solution of
(42)
(43)
(44)
Stationary Watermark t = 0.6
IID Watermark
No Watermark
IID Watermark
Stationary Watermark t = 0.6
k
b
0.3
0.2
0.1
0
-10
0
10
30
40
50
2
4
6
8
10
20
k
f igure 8 bk versus time k for a stationary watermark with
t = 0.6, an iid watermark, and no watermark. For watermarking
schemes, DJ = 10, and a = 0.02 for all schemes. When the attack
begins at time k = 0, the detection rate quickly increases due to a
mismatch in the expected and received measurements before
converging quickly to the asymptotic detection rate. Without
watermarking, the asymptotic detection rate is the false alarm
rate. Since the rate of detection quickly converges to its asymptotic
value for each design, it is reasonable to design a watermark
to optimize asymptotic detection performance.
maximize tr6F2 (~, H) CT P-1 C@
~,H
subject to F1 (~, H) # d, 0 # ~ # 0.5,
H Hermitian and positive semidefinite,
(45)
where the function F1 is defined as
F1 (~, H) _ tr6UH2@ + tr6(W + LT UL) H3@ ,
H2 _ 20e"2 sym (stL [I - st (A + BL)] -1 BH) +H,,
H3 _ 20e"2 sym6(I - st (A + BL)) -1 L1 (H)@ - L1 (H),,
and s _ exp (2rj~) . The function F2 is defined as
F2 (~, H) _ 20e"2 sym6(I - stA) -1 L2 (H)@ - L2 (H),. (47)
Furthermore, one optimal (not necessarily unique) H)
of (45) is of the form
H) = hhH,
where h ! Cp . The corresponding HMM is given by
cos 2r~) -sin 2r~)
pk+1 = t;sin 2r~) cos 2r~) Epk + }k,
gk = 6 2 hr
2 hi@pk,
where hr, hi ! Rp are the real and imaginary part of h,
respectively, and W = Cov (}k) = (1 - t2) I.
(46)
(48)
(49)
kn 30
o
it
c
e
t
e
fD 20
o
e
m
i
T
ted 10
c
e
p
x
E
0
0
9J
f igure 9 The expected time of detection versus DJ for a stationary
watermark with t = 0.6, and an independent and identically distributed
(iid) watermark. The probability of false alarm is fixed to be
a = 0.02. In the absence of physical watermarking, which corresponds
to DJ = 0, the expected time of detection is k = 34.3. However,
physical watermarks can significantly reduce the expected
time of detection. For instance, when DJ = 10, the expected time of
detection for the iid watermark is k = 6.27 and the expected time of
detection for the stationary watermark with t = 0.6 is k = 5.82.
Moreover, for the given range of DJ, the stationary watermark with
t = 0.6 allows the detector to identify replay attacks on average
earlier than in the case of an iid watermark.
Remark 11
By (44), C) (d) can be seen as a sinusoidal signal with a decay
factor t, where ~) and H) can be interpreted as the optimal
frequency and direction, respectively. Since F1 and F2
are linear with respect to H, when ~ is fixed, (45) is a semidefinite
programing problem and hence can be solved efficiently.
Therefore, (45) can be solved in two steps by first
calculating the optimal signal direction for every frequency
0 # ~ # 0.5 and then searching over all possible frequencies
~. In practice, (45) can be solved for enough sample
frequencies to obtain a near-optimal watermarking signal.
It is worth noticing that regardless of the dimensions of
the physical system n or the control input p, the dimension
of the hidden pk is always two, which is desirable from a
computational perspective when dealing with a highdimensional
linear system.
NuMERICAL Ex AMp LE
This section illustrates the utility of the watermarking
scheme by analyzing detection performance on a control
system, with parameters
1 1 0
A = ;0 1E, B = ;1E, C = 61 0@.
(50)
The cost matrices in this system, W and U, are equal to
the identity matrix. The covariance matrices, Q and R, are
equal to 0.8 times the identity matrix and the identity matrix,
respectively. As a result, the eigenvalues of A are -0.339
and -0.105. Consequently, A is stable, thus motivating
FEBRUARY 2015 « IEEE CONTROL SYSTEMS MAGAZINE 107
It was noted that for some control systems, the classical estimation,
control, and failure detection strategy is not resilient to a replay attack.
the use of a watermark signal for detection. Two watermarking
designs are analyzed. First, a stationary watermark is
generated using (49), where t = 0.6. In the second case, an
iid Gaussian process is considered, similar to the design presented
in [13] and [14]. Designing a stationary Gaussian
watermark requires solving a semidefinite program for a
set of frequencies sampled in 0 # ~ # 0.5. A step size of 0.01
is chosen for this system, which requires solving 51 semidefinite
programs. On a Macbook Pro with a 2.4-GHz processor,
solving all 51 semidefinite programs takes 12.9 s
using CVX [21], [22].
First, the asymptotic detection rate limk " 3 bk versus the
false alarm rate a for each design is plotted in Figure 3. The
additional cost DJ imposed by the watermark is ten for
each design, roughly 40% of the optimal cost J* = 23.1. The
relationship between the asymptotic detection rate and
false alarm rate is again considered in Figure 4. Here, a is
chosen to be under 0.1, which is typical for real systems,
where the cost considerations of investigating possible
attacks make it undesirable to have frequent false alarms
during normal operation. The stationary watermarking
design offers a visible improvement in the asymptotic rate
of detection over an iid design. The percent improvement
in asymptotic detection rate limk " 3 bk of the stationary
Gaussian design with t = 0.6 over the iid approach is
explicitly examined in Figure 5 for a # 0.1. It can be seen
that the stationary watermark achieves its best relative performance
for a in this range. In fact, a 60% improvement
over the iid design in the asymptotic rate of detection is
obtained when a . 0.005 and t = 0.6.
Figures 6 and 7 illustrate the tradeoff between the
asymptotic detection rate limk " 3 bk and the LQG cost DJ
for DJ # 100 and DJ # 20, respectively. For this simulation,
the false alarm rate a is fixed to be 0.02. For practical systems,
DJ needs to be carefully chosen to balance the control
cost and the detection performance.
Figure 8 shows the detection rate as a function of time k
where DJ = 10 for the watermarking approaches and
a = 0.02. In this scenario, detection performance in the
absence of physical watermarking is also considered. For
this case, a |2 detector is used. It is assumed that the
attacker gathers measurements from -50 # k # -1 and
replays these measurements from 0 # k # 49. For all chosen
designs, the probability of detection quickly rises to a maximum
detection rate at k = 0 due to a mismatch between
the expected and received measurements at the beginning
of a replay attack. However, since A is stable, the detection
rate quickly decreases back to false alarm rate without
108 IEEE CONTROL SYSTEMS MAGAZINE » FEBRUARY 2015
watermarking. Meanwhile, in the watermarking strategies
bk converges quickly. As a result, it is reasonable to design
the watermark signal to optimize the asymptotic detection
performance.
Finally, Figure 9 examines the relationship between the
expected time of detection and the additional LQG cost DJ
when a = 0.02. In the absence of physical watermarking,
which corresponds to DJ = 0, the expected time of detection
is roughly given by k = 34.3. Watermarking strategies
can significantly reduce the time of detection. For instance,
for DJ = 10, the expected time of detection for the stationary
watermark is k = 5.82 and the expected time of detection
for the iid watermark is k = 6.27.
CONCLu SION
This article defined a replay attack against a control system.
Specifically, the adversary could record a sequence of
sensor measurements and later deliver these previous outputs
to the system operator. If the system is operating in
steady state, it was shown that the replayed outputs are statistically
identical to the outputs of the system under
normal operation. It was noted that for some control systems,
the classical estimation, control, and failure detection
strategy is not resilient to a replay attack. In these systems,
an authenticating watermarked input was superimposed
on the LQG optimal input, providing improved detection
at the expense of control performance. The watermarked
input was assumed to be stationary and Gaussian, extending
previous results, which only considered the iid case. An
optimal Neyman-Pearson detector was given to determine
if the system is under attack. Furthermore, a method to
select the statistical properties associated with the watermark
was provided based on the tradeoff between desired
detection performance and allowable control performance
loss. In addition, an algorithm to generate a watermark
with these statistical properties was provided. Simulations
were carried out to examine asymptotic detection performance
as a function of the rate of false alarms, asymptotic
detection performance as a function of cost to the system,
detection performance as a function of time, and expected
time of detection as a function of cost. Future work consists
of applying the watermarking techniques to detect other
attacks, where the adversary cannot perfectly replicate the
effect of the watermark in the sensor measurements.
Au Th OR INf ORMATION
Yilin Mo (yilinmo@caltech.edu) is a postdoctoral researcher
in the Department of Control and Dynamical Systems
Future work consists of applying the watermarking techniques to detect
other attacks, where the adversary cannot perfectly replicate the effect
of the watermark in the sensor measurements.
at the California Institute of Technology. He received the
Ph.D. degree in electrical and computer engineering from
Carnegie Mellon University in 2012 and the bachelor of
engineering degree from the Department of Automation,
Tsinghua University, in 2007. His research interests include
secure control systems and networked control systems,
with applications in sensor networks. He can be contacted
at the Department of Computing and Mathematical Sciences,
California Institute of Technology, 1200 E. California
Blvd., Pasadena, CA 91125 USA.
Sean Weerakkody received the B.S. degree in electrical
engineering and mathematics from the University of Maryland,
College Park, in 2012. He is currently working toward
the Ph.D. degree at the Electrical and Computer Engineering
Department, Carnegie Mellon University, Pittsburgh,
Pennsylvania. His research interests include security in
cyberphysical systems and distributed estimation with applications
to sensor networks.
Bruno Sinopoli received the Dr. Eng. degree from the
University of Padova in 1998 and the M.S. and Ph.D. degees
in electrical engineering from the University of California,
Berkeley, in 2003 and 2005 respectively. After a postdoctoral
position at Stanford University, he joined the faculty at Carnegie
Mellon University, where he is an associate professor
in the Department of Electrical and Computer Engineering
with courtesy appointments in Mechanical Engineering and
in the Robotics Institute. He was awarded the 2006 Eli Jury
Award for outstanding research achievement in the areas of
systems, communications, control, and signal processing at
the University of California, Berkeley, the 2010 George Tallman
Ladd Research Award from Carnegie Mellon University,
and the NSF Career award in 2010. His research interests
include networked embedded control systems, distributed
estimation, and control with applications to wireless sensoractuator
networks and physical systems security.
REf ERENCES
[1] R. Langner. (2013, Nov.). To kill a centrifuge: A technical analysis of what
Stuxnet's creators tried to achieve. Langner Communications. Tech. Rep.,
[Online]. Available: www.langner.com/en/wpcontent/uploads/2013/11/
To-kill-a-centrifuge.pdf
[2] A. Willsky, “A survey of design methods for failure detection in dynamic
systems,” Automatica, vol. 12, pp. 601-611, Nov. 1976.
[3] A. Abur and A. G. Expósito, Power System State Estimation: Theory and
Implementation. Boca Raton, FL: CRC Press, 2004.
[4] Y. Liu, M. Reiter, and P. Ning, “False data injection attacks against state
estimation in electric power grids,” ACM Trans. Inform. Syst. Secur., vol. 14,
no. 1, pp. 13:1-13:33, 2011.
[5] H. Sandberg, A. Teixeira, and K. H. Johansson, “On security indices for
state estimators in power networks,” in Proc. 1st Workshop Secure Control
Systems, Stockholm, Sweden, 2010, paper 10.
[6] F. Pasqualetti, F. Dorer, and F. Bullo, “Attack detection and identification
in cyber-physical systems,” IEEE Trans. Autom. Contr., vol. 58, no. 11, pp.
2715-2729, 2013.
[7] H. Fawzi, P. Tabuada, and S. Diggavi, “Secure estimation and control
for cyber-physical systems under adversarial attacks,” IEEE Trans. Autom.
Contr., vol. 59, no. 6, pp. 1454-1467, 2014.
[8] S. Sundaram, M. Pajic, C. Hadjicostis, R. Mangharam, and G. J. Pappas,
“The wireless control network: Monitoring for malicious behavior,” in Proc.
IEEE Conf. Decision Control, Atlanta, Georgia, Dec. 2010, pp. 5979-5984.
[9] S. Sundaram and C. N. Hadjicostis, “Distributed function calculation via
linear iterative strategies in the presence of malicious agents,” IEEE Trans.
Autom. Contr., vol. 56, no. 7, pp. 1495-1508, 2011.
[10] F. Pasqualetti, A. Bicchi, and F. Bullo, “Consensus computation in unreliable
networks: A system theoretic approach,” IEEE Trans. Autom. Contr.,
vol. 57, no. 1, pp. 90-104, Jan. 2012.
[11] Y. Mo and B. Sinopoli, “False data injection attacks in cyber physical
systems,” in Proc. 1st Workshop Secure Control Systems, Stockholm, Sweden,
Apr. 2010, paper 7.
[12] Y. Mo, E. Garone, A. Casavola, and B. Sinopoli, “False data injection
attacks against state estimation in wireless sensor networks,” in Proc. 49th
IEEE Conf. Decision Control, Atlanta, GA, 2010, pp. 5967-5972.
[13] Y. Mo and B. Sinopoli, “Secure control against replay attacks,” in Proc.
47th Annu. Allerton Conf. Communication, Control, Computing, Allerton, IL,
2009, pp. 911-918.
[14] Y. Mo, R. Chabukswar, and B. Sinopoli, “Detecting integrity attacks
on SCADA systems,” IEEE Trans. Control Syst. Technol., vol. 22, no. 4, pp.
1396-1407, 2014.
[15] P. Kumar and P. Varaiya, Stochastic Systems: Estimation, Identification, and
Adaptive Control. Englewood Cliffs, NJ: Prentice-Hall, 1986.
[16] R. K. Mehra and J. Peschon, “An innovations approach to fault detection
and diagnosis in dynamic systems,” Automatica, vol. 7, no. 5, pp. 637-640,
Sept. 1971.
[17] P. E. Greenwood and M. S. Nikulin, A Guide to Chi-Squared Testing. New
York: Wiley, Apr. 1996.
[18] L. L. Scharf and C. Demeure, Statistical Signal Processing: Detection, Estimation
and Time Series Analysis. Reading, MA: Addison-Wesley, 1991.
[19] T. Chonavel and J. Ormrod, Statistical Signal Processing: Modelling and
Estimation (Series Advanced Textbooks in Control and Signal Processing).
Berlin Heidelberg, Germany: Springer-Verlag, 2002.
[20] P. Delsarte, Y. Genin, and Y. Kamp, “Orthogonal polynomial matrices
on the unit circle,” IEEE Trans. Circuits Syst., vol. 25, no. 3, pp. 149-160, 1978.
[21] M. Grant and S. Boyd. (2013, Sept.). CVX: Matlab software for disciplined
convex programming, version 2.0 beta. [Online]. Available: www.
cvxr.com/cvx
[22] M. Grant and S. Boyd, “Graph implementations for nonsmooth convex
programs,” in Recent Advances in Learning and Control (Series Lecture
Notes in Control and Information Sciences, vol. 371), V. Blondel, S. Boyd,
and H. Kimura, Eds. Berlin Heidelberg, Germany: Springer-Verlag, 2008,
pp. 95-110.
FEBRUARY 2015 « IEEE CONTROL SYSTEMS MAGAZINE 109