49th IEEE Conference on Decision and Control
December 15-17, 2010
Hilton Atlanta Hotel, Atlanta, GA, USA
Optimal Control in the Presence of an Intelligent Jammer with Limited
Actions
Abhishek Gupta
Ce´dric Langbort
Tamer Bas¸ar
Abstract- We consider a dynamic zero-sum game between
two players. The first player acts as a controller for a discrete
time LTI plant, while the second player acts to jam the
communication between the controller and the plant. The
number of jamming actions is limited. We determine saddlepoint
equilibrium control and jamming strategies for this game
under the full state, total recall information structure for
both players, and show that the jammer acts according to
a threshold policy at each decision step. Various properties
of the threshold functions are derived and complemented by
numerical simulation studies.
I. INTRODUCTION
One of the main challenges associated with networked
control systems is the presence of a communication channel
between the controller and the plant. This channel introduces
a number of limitations in the forward path of the control
loop, which restricts the controller's ability to stabilize the
system or achieve optimality in closed-loop.
Examples of such limitations include finite rate and
channel capacity, stochastic packet drops and delays, and
bounded signal-to-noise ratio. In addition, in cases where
the controller receives no acknowledgment from the channel,
and hence has no information as to whether or not the input
has reached the plant, additional difficulties arise in the form
of so-called non-standard information patterns (see, e.g., [1],
[2] for recent special issues devoted to these issues). The
effects of such communication channel-induced limitations
on control have been intensively studied in the past decade.
For example, a number of papers have considered the minimum
channel capacity or rate necessary for stabilization
(see, e.g. [3], [4]) or achieving optimal quadratic closed-loop
performance [5], [6], [7].
In all the works mentioned above, the channel effects are
treated as exogenous in the sense that the channel behavior
is independent from the controller's actions and control task
(for example, delays are typically not assumed to result from
the rate at which the controller communicates with the plant).
In particular, the degradation of the controller's output signal
is not strategic or malicious. While this type of model is
appropriate for problems of control over the Internet, it may
not be so when the channel is a dedicated wireless link that is
susceptible to jamming and/or in military scenarios where an
adversary might be specifically interested in disrupting the
communication between, say, a base-station and a remote
vehicle.
All authors are with the Coordinated Science Laboratory, University
of Illinois at Urbana-Champaign, Urbana, IL 61801; emails: {gupta54,
langbort, basar1}@illinois.edu. Research was partially supported
by AFOSR Grant FA9550-09-1-0249.
978-1-4244-7746-3/10/$26.00 ©2010 IEEE
1096
In this paper, we consider the case of a strategic jammer,
whose goal is to actively and optimally perturb the control
process by using a finite number of jamming actions over
a horizon of N time steps. This constraint on the number
of jamming actions is similar to that introduced in [8], [9]
in the case of optimal control (without an adversary), and in
[10] in the case of estimation (again without an adversary). It
is introduced in the present problem to capture the fact that,
since jamming is a power intensive activity and available
power on-board a jammer is typically limited, continuous
action throughout the entire decision horizon is impossible.
Our formulation, detailed in Section II, naturally results
in a dynamic zero-sum game between the jammer and the
controller. We show that saddle-point equilibrium strategies
exist and use dynamic programming to compute them. In
particular, we show that the jammer saddle-point equilibrium
strategy is threshold-based, which means that at every time
step, the jammer jams if and only if the plant's state is larger
than an off-line computable and time-varying threshold. We
start by investigating a simple situation in Section III, in
which the jammer can only act once over a 3-steps horizon.
We derive the threshold functions analytically in this case.
The case of general N is then treated in Section IV. Finally,
in Section V, we indicate further directions for research on
the problem of optimal control in the presence of a strategic
jammer, both within and beyond the framework and models
introduced in this paper.
II. PROBLEM FORMULATION
The class of problems considered in this paper can
be viewed as the standard discrete-time linear-quadraticGaussian
(LQG) control problem with state feedback, but
with one major difference: as a networked control system,
the link connecting the output of the controller to the plant is
unreliable due to the presence of adversarial jamming, with
a possibility of the control signal being intercepted by the
jammer and not reaching the plant. Instead of limiting the
jammer's action through an energy constraint, we allow the
jammer only M possibilities of interception on problem of
horizon N , where M < N . We further assume that if the
control signal is intercepted, the input to the plant is zero.1
Using scalar system dynamics, the scenario above can be
captured through the following mathematical formulation:
1It would be possible to adopt an alternate formulation where whenever
the control signal is intercepted, the actuator generates an input that is based
on the most recently received control signal, but this will not be pursued in
this paper.
The state equation under adversarial jamming evolves as
xk+1 = Axk + αkuk + wk ,
k = 0, 1, ..., N − 1 , (1)
where xk ∈ R is the state of the plant, uk ∈ R is the control
signal, {wk} is a discrete-time zero mean Gaussian white
noise process with variance σw2 (i.e. wk ∼ N (0, σw2), and
x0 is also a zero mean Gaussian variable, with variance σ2,
0
and independent of {wk} . The sequence {αk ∈ {0, 1}} is
the control of the jammer, where αk = 0 means that the
jammer is active at time k, whereas αk = 1 means that the
jammer is inactive and the control signal reaches the plant.
The assumption that the jammer is allowed to intercept at
most M times (in a horizon of N ), is captured by the jammer
constraint PkN=−01(1 − αk) = M . Note that here we actually
use an equality rather than an inequality because, as will
become clear later in our analysis, there is no incentive for
the jammer not to use all M allotments for interception, since
it does not incur any cost during each jamming instance.
The cost function associated with this problem is
J = E
(N −1
X (x2k + αkuk) + xN
2 2
k=0
)
(2)
which is to be minimized by the controller and maximized by
the jammer. Note that when the control signal is intercepted
(that is, αk = 0), the controller accrues no cost for control.
This is clearly a zero-sum dynamic game, but to make
the problem precise we have to specify the underlying
information structure, and the equilibrium solution concept to
be adopted. Toward this end, let x[0,k] := {x0, . . . , xk},with
a similar definition applying to α[0,k], and let us introduce
I0 := {x0} , Ik := {x[0,k], α[0,k−1]} for k ≥ 1
as the information available to both the controller and the
jammer at time k. We introduce control policies (strategies)
for the controller and the jammer as measurable mappings,
{γk} and {μk}, respectively, from their information sets
(which are the same for both) to their action sets; more
precisely, uk = γk(Ik) and αk = μk(Ik), where
γk : Rk+1×{0, 1}k → R and μk : Rk+1×{0, 1}k → {0, 1} .
We further restrict μ := {μ0, . . . , μN −1} to those maps that
satisfy the jammer constraint, with αk = μk(Ik); let us
denote the class of all such policies for the jammer by M and
for controller by Γ. At each point in time, the controller has
access to the current value of the state, recalls the past values,
and also has full memory on whether any of the previous
control signal transmissions were intercepted or not. This
latter information could be made available to the controller
through acknowledgement messages sent from the plant, as
in TCP of the Internet. Likewise, the jammer has access
to full state information, and recalls its past actions. There
could, of course, be various variations of this information
structure. Given the information structure introduced above,
and the feasible policies of the controller and the jammer, we
rewrite the cost function as J (γ, μ), in terms of the policies γ
and μ, and seek a pair (γ∗ ∈ Γ, μ∗ ∈ M) with the property:
J (γ∗, μ) ≤ J (γ∗, μ∗) ≤ J (γ, μ∗) ∀γ ∈ Γ, μ ∈ M .
This is a saddle-point solution for the underlying game,
where the controller is the minimizer and the jammer the
maximizer, and the order in which they determine their
policies is immaterial (that is, the upper and lower values
are equal). Of course, this has not been established as yet,
and one of the goals of the paper is to show that this is
indeed the case, and also to obtain the saddle-point solution.
When M = 0, this is precisely the standard LQG problem
with perfect state measurements, and for M = N , the
controller signal is always intercepted and hence any pair
of the form (γ, 0) with γ ∈ Γ is trivially a saddle-point
solution; it is the intermediate case that is of interest.
A. Extended game and solution approach
In order to establish the existence of and compute saddlepoint
equilibrium strategies, it is easiest to extend the game's
state space so as to keep track of the jammer's options at
a particular time step, and redefine the dynamics on this
state space. An extented state of the dynamic zero-sum
game defined by cost function J , information sets {Ik}
and evolution equation (1) is a triple (x, s, t) ∈ E :=
R × {0, ..., M } × {0, ..., N }, where x can be thought of
as the state of the controlled plant, t = N − k can be
thought of as the number of remaining decision steps, and
s can be thought of as the number of remaining jamming
instances available to the jammer. We will also say that “x
is the state of the plant at stage (s, t)” and sometimes write
x(s,t) to denote this. We will denote the jammer's action
space at stage (s, t) by A(s,t) ⊂ {0, 1}. From an extended
state (x, s, t) ∈ E such that A(s,t) = {0, 1}, the system can
transition to two extended states, depending on the jammer's
and controller's action at that state: (Ax + u + w, s, t − 1)
or (Ax + w, s − 1, t − 1). The first state is reached when
the controller is applying input u and the jammer is inactive
(α = 1), while the second is reached when the jammer is
active (α = 0), regardless of the controller's action. When
A(s,t) is a strict subset of {0, 1}, only one of those two
transitions is possible. The projection of the extended state
space onto the (s, t)− space thus has the structure of the
graph of Figure 1.
C
(0, 3)
C
(0, 2)
C
(0, 1)
J C
(1, 3)
(0, 0)
J C
(1, 2)
J
(1, 1)
J C
(2, 3)
J
(2, 2)
F
o
r
w
a
r
d
i
n
it
m
e
J
(3, 3)
Fig. 1. A portion of the extended state space. Here 'J' denotes that the
jammer is active in that stage and 'C' denotes that the jammer is idle (and
control signal is received by the plant). Depending on the value of M , some
of the depicted transitions may not be possible.
1097
The original zero-sum dynamic game introduced previously
naturally induces a zero-sum dynamic game on the
extended state space E by keeping the same cost function J
as in (2) and using the extended state transition rule defined
above. A controller feedback policy on E is a map γ˜ : E → R
and, likewise, a jammer feedback policy on E is a map
μ˜ : E → {0, 1}. Given a controller policy γ˜ on E , we can
define a feasible policy γ ∈ Γ for the original game by
γk(x[0,k], α[0,k−1]) :=
γ˜(xk, M − card{i ∈ [0, k − 1] |αi = 0 }, N − k)
for all k. Similarly, we can associate a jammer policy
μ ∈ M to a feedback jammer policy on E . As a result,
if the zero-sum game defined on the extended state space
has a saddle-point equilibrium in feedback strategies, the
original zero-sum game admits a saddle-point equilibrium
(γ∗ ∈ Γ, μ∗ ∈ M). Note, however, that the converse
may not be true, as a feasible strategy γ for the original
game does not always uniquely correspond to a feedback
strategy γ˜ on E (since, e.g., some γk could depend on
the exact jamming sequence α[0,k−1] instead of just the
number of jamming events so far). As a first approach to the
problem of control in the presence of an intelligent jammer,
we focus exclusively on saddle-point equilibrium strategies
corresponding to feedback strategies defined on E in this
paper. A more complete characterization of all saddle-point
strategies in Γ × M will be the subject of future work.
A straightforward generalization of Corollary 6.2 on page
282 of [11], establishes that strategies γ˜∗ and μ˜∗ are feedback
saddle-point equilibrium strategies defined on E if and only
if, for all (s, t) ∈ {0, ..., M }×{0, ..., N } there exist functions
V(s,t) : R → R such that the following recursive equations
hold for all x ∈ R:
V(0,0)(x) = x2,
V(s,t)(x) = inf max
u α∈A(s,t)
E{x2 + αu2+
V(s+(α),t−1)(Ax + αu + w)} . (3)
In (3), we have let s+(α) = s −s 1 iiff αα == 01 .
In the remaining sections of this paper, we explicitly
compute such functions V(s,t), thus effectively and constructively
proving the existence of feedback saddle-point
equilibrium strategies defined on E , and, in turn, of saddlepoint
equilibrium strategies in Γ × M for the original game.
At this point, it is worth emphasizing that the equality
between inf-max and max-inf does indeed hold in (3), i.e.,
that the game has a value. This follows directly from the
fact that the function u 7→ E{x2 + V(s−1,t−1)(Ax + w)}
(which appears in the right hand-side of (3) when α = 0) is
a constant and the following lemma.
Lemma 1: Let f be a function and M be a constant. Then
inf max(f (u), M ) = max (inff (u), M ).
u u
Proof: Let U := {u|f (u) < M }. Then we have two
cases: U = ∅ and U 6= ∅. When U = ∅,
f (u) ≥ M for all u.
hence max(f (u), M ) = f (u) for all u and
inf max(f (u), M ) = inff (u). Besides, inequality
u u
(4) also implies that inff (u) ≥ M , so that
u
max (inff (u), M ) = inff (u). Now, if U 6= ∅, then
u u
inff (u) < M and max (inff (u), M ) = M . On the other
u u
hand, by definition, max (f (u), M ) ≥ M for all u and, since
U =6 ∅, there exists u0 such that max (f (u0), M ) = M .
Hence, inf max(f (u), M ) = M .
u
B. Notation
We now introduce some notations. We denote the jammer's
and controller's best response costs at stage (s, t), respectively
as
J(s,t)(x, u, α) := E{x2 + αu2 +
J
J(s,t)(x) :=
J(Cs,t)(x) :=
V(s+(α),t−1)(Ax + αu + w)},
x2 + E{V(s−1,t−1)(Ax + w)},
infE{x2 + u2 + V(s,t−1)(Ax + u + w)}.
u
With these notations, feedback saddle-point equilibrium
strategies defined on E are characterized by the fact that,
when the plant state is x at stage (s, t), the controller's
action minimizes J(s,t)(x, u, α) over u, while the jammer is
choosing the action corresponding to the largest of the two
costs between J(Cs,t)(x) and J(Js,t)(x) when A(s,t) = {0, 1}.
As we will see, this results in a threshold-based policy in
which the action of the jammer at (s, t) depends on the
sign of the quantity |x| − τ(s,t)(x) for an off-line computable
threshold function τ(s,t)(x).
Another object that we will make frequent use of in the
subsequent sections is the conditional probability density
function of the state at a given stage. When a transition from
stage (s, t) to stage (s′, t′) is possible in Figure 1, and control
action u is applied at stage (s, t), we denote this conditional
probability density function of the state x(s′,t′) given the
state x(s,t) and u by f (x(s′,t′)|x(s,t), u). If the jammer was
inactive during the stage (s, t), then s′ = s, t′ = t − 1,
and x(s′,t′) = Ax(s,t) + u + wk. Since the noise wk is an
i.i.d. Gaussian random variable, the conditional probability
density function follows a normal distribution, given by
f x(s,t−1)|x(s,t), u = N
Ax(s,t) + u, σw2 .
If the jammer is active at stage (s, t), s′ = s − 1, t′ = t − 1,
and x(s′,t′) = Ax(s,t) +wk so that the conditional probability
density function is
f x(s−1,t−1)|x(s,t), u = N
Ax(s,t), σw2 .
(5)
(6)
Note that it does not depend on control action u in this case.
III. THE M = 1, N = 3 CASE
In order to illustrate the main steps of our derivations
while keeping notation to a minimum, we start by computing
feedback saddle-point equilibrium strategies (γ˜∗, μ˜∗) for the
extended game in the simple case where N = 3 and M = 1
(i.e., the jammer can only jam once in three time steps). By
(4)
1098
definition, V(0,0)(x) = x2. At the next step, we can be in
either of the two stages (0, 1) and (1, 1), depending upon
whether the jammer was active in the last decision period or
not (see Figure 1). At stage (0, 1), the jammer has no chance
left to jam and his action space is reduced to A(0,1) = {1}.
The jammer best response cost is thus
J(0,1)(x, u) = E{(Ax + u + w2)2 + x2 + u2}.
(7)
Using the first order necessary condition for optimality, we
find that the optimal control action γ˜∗(x, 0, 1) satisfies
∂J(0,1) = 2(Ax + γ˜∗(x, 0, 1)) + 2γ˜∗(x, 0, 1) = 0,
∂u
i.e., γ˜∗(x, 0, 1) = − A2 x. The value function at this stage is
A2
V(0,1)(x) =
1 +
x2 + σw2.
2
In stage (1, 1), the jammer must always jam, otherwise the
jammer constraint is violated. The value function at (1, 1) is
J 2
V(1,1)(x) = J(1,1)(x) = (1 + A2)x2 + σw.
(8)
(9)
Let us now move on to stages (0, 2) and (1, 2). Note that
the noise w1 in these stages is independent from the noise
w2 occurring in the next stage. Applying the same approach
as above, we find that the optimal control for stage (0, 2) is
1 + A2 !
γ˜∗(x, 0, 2) = −A 2 x
2 + A2
2
and that the corresponding value function is given by
V(0,2)(x) =
1 + A2 x2
+
2 +
2
σw. (11)
2A2
4 + A2
A2
2
Define κ(10,C,2) = 1 + A2 − 42+AA22 and κ(20,C,2) = 2 + A22 .
The case of stage (1, 2) requires more effort since the jammer
has two options, i.e., A(1,2) = {0, 1}. The controller's best
response costs are found to be
J(J1,2)(x) = κ(11,J,2)x2 + κ(21,J,2)σw2
J(C1,2)(x) = κ(11,C,2)x2 + κ(21,C,2)σw2,
where κ(11,J,2) = 1 + A2
1 +
, κ(21,J,2) = 2 +
κ(11,C,2) = 1 + A2 ,
κ(21,C,2) = 2 + A2
If the difference between these costs J(J1,2)(x) − J(C1,2)(x) ≥
0, then the jammer must jam. Hence, the value function is
A2
2
V(1,2)(x) =
( J(J1,2)(x)
J(C1,2)(x)
if |x| ≥ τ(1,2)
if |x| < τ(1,2)
A2
2
A2
2 + A2
r
0
1
where we defined τ(1,2) := A4+2+2AA22+2
saddle-point equilibrium strategies (γ˜∗, μ˜∗) is
σw. The feedback
μ˜∗(x, 1, 2) =
and γ˜∗(x, 1, 2) = −A
if |x| ≥ τ(1,2) ,
if |x| < τ(1,2)
1 + A2
x ∀ x.
2 + A2
Let us now consider stage (1, 3), the initial stage. The
controller's cost if the jammer decides to jam at this stage is
J(J1,3)(x) = κ(11,J,3)x2 + κ(21,J,3)σw2
where κ(11,J,3) =
1 + A2κ(10,C,2) , κ(21,J,3) = κ(10,C,2) + κ(20,C,2)
E(V(1,2)(x1)) =
To compute the controller's best response cost when the
jammer is idle, J(C1,3), we need to calculate E(V(1,2)(x1)),
where x1 = Ax + u + w0 for a given controller action
u. According to (12), and recalling the definition of f (.|.)
introduced in Section II-B, we see that
Z
f (x1|x, u)J(J1,2)(x1)dx1
2The second order condition for infimum is difficult to evaluate at the
optimal control γ˜∗(x, 1, 3) analytically. However, in all our simulations,
we observed that γ˜∗(x, 1, 3) is the unique minimum.
Let us introduce P(1,3)(x, u, τ(1,2)) as the conditional
probability that |x(1,2)| lies above the threshold τ1,2, given
that x(1,3) = x and the control action at stage (1, 3) is u,
Z
|x1|≥τ(1,2)
P(1,3)(x, u)
=
f (x1|x, u)dx1.
(16)
Let us also write P (1,3)(x, u) = 1 − P(1,3)(x, u) for the
conditional probability that |x(1,2)| < τ(1,2), and introduce
the following two second moments of x1
R|x1|≥τ(1,2) x21f (x1|x,u)dx1
(17)
(Ax + u)2 + σw2
and R(1,3)(x, u) := 1 − R(1,3)(x, u). The cost at stage (1, 3)
with control is
J(C1,3)(x) = inuf x2 + u2 + E(V(1,2)(Ax + u + w0)) . (18)
The infimum in (18) is attained and the corresponding
minimum point is γ˜∗(x, 1, 3). To compute this minimum2,
0 = H (x) = ∂ x2 + u2 + E(V(1,2)(Ax + u + w0)) (19)
∂u
which gives an implicit equation characterizing γ˜∗(x, 1, 3).
Now, letting L(1,3)(x) := −γ˜∗(x, 1, 3)/(Ax) and plugging
the obtained value of γ˜∗(x, 1, 3) back into (18), yields
J(C1,3)(x) = κ(11,C,3)(x)x2 + κ(21,C,3)(x)σw2
(20)
where κ(11,C,3)(x) and κ(21,C,3)(x) is obtained by putting u =
−L(1,3)(x)Ax in (18). Once both functions J(J1,3) and J(C1,3)
have been determined, the value function at stage (1, 3) is
V(1,3)(x) =
( J(J1,3)(x)
J(C1,3)(x)
if |x| ≥ τ(1,3)(x)
if |x| < τ(1,3)(x)
,
(21)
where the threshold function τ(1,3)(x) is defined such that
J(J1,3)(x) − J(C1,3)(x) ≥ 0 if and only if |x| ≥ τ(1,3)(x).
Analytically, we find that
τ(1,3)(x)
=
vu κ(21,C,3)(x) − κ(21,J,3) σw.
u
t κ(11,J,3) − κ(11,C,3)(x)
(22)
Z
+
|x1|≥τ1,2
|x1|<τ1,2
f (x1|x, u)J(C1,2)(x1)dx1.
(15)
(10)
R(1,3)(x, u) =
(12)
(13)
(14)
1099
Note that, unlike τ(1,2), threshold function τ1,3 is not constant,
and that its computation requires determining γ˜(., 1, 3).
Also note that κ(11,C,3) and κ(21,C,3) are even functions, i.e.,
that κ(11,C,3)(−x) = κ(11,C,3)(x) and κ(21,C,3)(−x) = κ(21,C,3)(x).
This is because P(1,3)(−x, −u) = P(1,3)(x, u) and the same
property holds for R(1,3)(x, u). As a result, τ(1,3)(x) is even.
We are now in a position to prove two results, which give
us an insight into the nature of threshold function τ(1,3).
Proposition 1: L(1,3)(x) 6= 1 ∀ x 6= 0
Proof: The proof is by contradiction. If L(1,3)(x) = 1, then
the right hand-side of (19) vanishes when u = −Ax. But,
the derivative of R(1,3) and P(1,3) with respect to u is zero
at u = −Ax. Using this relation in (19) for x 6= 0, we get
H (x) = 2u = −2Ax 6= 0
Therefore, γ˜∗(x, 1, 3) 6= −Ax and L(1,3)(x) 6= 1 for all x.
Proposition 2: lim|x|→∞ τ(1,3)(x) exists and is finite.
Proof: From Lemma 1, we know that L(1,3)(x) 6= 1 for all
x 6= 0. Therefore, lim|x|→+∞ |Ax + γ˜(x, 1, 3)| = +∞ also
holds. Taking the limit as |x| → ∞ in (16)-(17), we obtain
lim
|x|→∞
P(1,3)(x, u) = 1,
lim
|x|→∞
R(1,3)(x, u) = 1,
Also, the derivative terms in (19) vanish as |x| → ∞.
Taking lim|x|→∞ A1x H (x) in (19) and rearranging, gives
lim|x|→∞ L(1,3)(x) = 1+κ(1κ1,(1J,12,J,)2) . Substituting this in (22),
lim
|x|→∞
v
u
u
τ(1,3)(x) = u
ut κ(11,J,3) which
proves the proposition.
κ(11,J,2) + κ(21,J,2) − κ(21,J,3)
1 + A2
κ(11,J,2)
1+κ(11,J,2)
σw
(23)
Figure 2 shows the graph of the threshold function τ(1,3)(x).
As predicted by Lemma 2, we observe that the threshold
τ(1,3)(x) reaches the limiting value given by (23) when the
state value x is sufficiently large. Also notice that when the
state is sufficiently large, the value of |x|−τ(1,3)(x) is greater
than 0, and it is beneficial for jammer to jam in this region.
Fig. 2. Graph of function τ(1,3)(x) for A = 2.5 and σw = 1. In
the dark-colored narrow strip, absolute value of state |x| is less than the
threshold τ(1,3)(x), while the reversed inequality holds in the white region.
The jammer is active at stage (1, 3) if x belongs to the white region.
IV. GENERAL CASE
Building on the intuition drawn from the results of Section
III, we can prove the following theorem regarding
the existence and characterization of feedback saddle-point
equilibrium strategies defined on E in the case where M = 1
and N is arbitrary. The result is proved by induction on t.
While the proof is omitted for reasons of space, the steps
and rationale are identical to those used in Section III.
Theorem 1: Let M = 1 and N > 1. Let coefficients be
defined according to the following recursion:
κ(10,C,0) = 1, κ(20,C,0) = 0,
κ(10,C,t−1)
1 + κ(10,C,t−1)
κ(10,C,t) = 1 + A2
; κ(20,C,t) = κ(10,C,t−1) + κ(20,C,t−1),
κ(11,J,t) = 1 + A2κ(10,C,t−1);
κ(21,J,t) = κ(10,C,t−1) + κ(20,C,t−1),
κ(11,C,t)(x) = 1 + A2
2
L(1,t)(x)
+(1 − L(1,t)(x))2ψ(11,t)(x, γ˜∗(x, 1, t))
κ(21,C,t)(x) = ψ(1,t)(x, γ˜∗(x, 1, t)) + ψ(1,t)(x, γ˜∗(x, 1, t)),
1 2
for all t ≥ 1 and all x, where, the set X(1,t) is defined as
X(1,t) = nx(1,t) ∈ R : x(21,t) − τ(21,t)(x(1,t)) ≥ 0o ,
the threshold τ(1,t)(x(1,t)) and ψ(x, u)'s are defined as
τ(1,t)(x(1,t))
=
1
ψ(1,t)(x, u)
2
ψ(1,t)(x, u)
vu κ(21,C,t)(x(1,t)) − κ(21,J,t) σw,
u
t κ(11,J,t) − κ(11,C,t)(x(1,t))
Z
=
Z
=
X(c1,t−1)
X(c1,t−1)
+R(1,t)(x, u)κ(11,J,t−1)
+P(1,t)(x, u)κ(21,J,t−1),
κ(11,C,t−1)(x¯)x¯2
(Ax + u)2 + σw2
f (x¯|x, u)dx¯
κ(21,C,t−1)(x¯)f (x¯|x, u)dx¯
conditioned probability and second moment defined as
P(1,t)(x(1,t), u(1,t)) = Pr{x(1,t−1) ∈ X(1,t−1)|x(1,t), u(1,t)}
R
R(1,t)(x(1,t), u(1,t)) =
X(1,t−1) x2f (x|x(1,t), u(1,t))dx
(Ax(1,t) + u(1,t))2 + σw2
and optimal control γ˜∗(x, 1, t) is
arg inf hx2 + u2 + (Ax + u)2ψ(11,t)(x, u)
u
+σw2
ψ(11,t)(x, u) + ψ(21,t)(x, u) i .
Then, the strategies (γ˜∗, μ˜∗) given below are feedback
saddle-point equilibrium strategies defined on E :
0
1
Aκ(10,C,t−1) !
1 + κ(10,C,t−1)
if x ∈ X(1,t)
if x ∈ X(c1,t)
x; μ˜∗(x, 0, t) = 1 ∀ t, x
γ˜∗(x, 1, t)
=
γ˜∗(x, 0, t)
μ˜∗(x, 1, t)
=
=
1100
1101